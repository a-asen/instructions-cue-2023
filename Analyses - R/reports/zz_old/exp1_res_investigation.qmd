---
title: "exp1_investigating_resutls"
format: docx
editor: 
  markdown: 
    warp: 62
---


```{r libraries }
#| include: FALSE
library(tidyverse)
library(ggpp)
library(gt)
library(patchwork)
library(BayesFactor)
library(bayestestR)
library(lsr)
source("../lib/helper_functions.R")
```

```{r load data }
#| include: FALSE

list.files("../data/raw/experiment1/task", pattern = "*.csv", full.names = T) -> fnames

map_df(fnames, \(x){
  read_csv(x)
}) -> data

read_csv("../data/raw/experiment1/demographics/demo.csv") -> demo
```

# Normal exclu
```{r transformation}
#| include: FALSE
# Pre-transformation
data |>
  mutate(rt = as.integer(ifelse( rt == "null", NA, rt ))) -> data
```

```{r}
# Select the relevant columns & rows
# Remove the first inducer_run (i.e., 0th)
data |>
  select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, congruent) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") |>
  # Filter the first inducer run (combined practice)
  filter(inducer_run != 0) -> d
```

## Acc
```{r exclusion start - accuracy}
#| include: FALSE
loss <- list()
loss$data_trials <- nrow(d)

### Overall accuracy      ====
d |>
  filter( trial_info == "Diagnostic trial" | trial_info == "Inducer trial" ) |>
  group_by( id ) |>
  mutate( correct_response = ifelse( trial_info=="Inducer trial" & is.na(rt), 0, correct_response) ) |>
    #' !  Non-responses count as a wrong response & are subject to the exclusion criteria.  !
  summarize( acc = sum(correct_response, na.rm = TRUE) / length( !is.na(correct_response) ) ) |>
  filter( acc < .7 ) |>
  pull( id ) -> loss$exclude_par

loss[["exclude_par_trials"]] <- length( d$rt[d$id == loss$exclude_par] )
loss[["exclude_par_pct"]]    <- length( d$rt[d$id == loss$exclude_par] ) / loss$data_trials * 100

d |> filter( !(id %in% loss$exclude_par) ) -> d
```

In total, `r length(loss[["exclude_par"]])`
`r ifelse(length(loss[["data_trials"]]) > 1, "participants", "participant")`
where excluded due to low accuracy (> 70%). Resulting in a
loss of `r loss[["exclude_par_pct"]]` percent of the data.

## sd*2.5
```{r due to high SD & NA responses }
#| include: FALSE

# Removing trials deviating more than 2.5 SD from each participants mean. 
# Also remove NA responses
d |>
  group_by(id) |>
  mutate(
    # Calculate: rt + SD * 2.5
    rt_crit = ifelse( 
      trial_info == "Diagnostic trial",
      mean( rt, na.rm = TRUE ) + sd( rt, na.rm = TRUE ) * 2.5,
      NA ),
      # Trials to retain/remove: 
      retain_trials = ifelse(
        # Remove trials deviating **more** than 2.5 SD
        (rt > rt_crit & trial_info == "Diagnostic trial") | # **OR**
          # Remove slow (i.e., NA) responses
          (is.na(rt) & trial_info == "Diagnostic trial"),
        # !! Supposed to be *not* missing !! 
        0, 1 ) 
    ) -> d

# IMPORTANT NOTE: 
  #' Removing null RT responses are, in all theory, not preregistered. 
  #' While there is a solution to "non-responses" for error (count them as wrong), 
  #' there is no stated solution to response time. Generally, this leads to problems 
  #' because a trial can be considered wrong under error conditions, while the accompanied 
  #' "error" for response time is not obvious. Should the RT be 2000ms (max trial length)? 
  #' Or as an infinity RT? 
  #' 
  #' Assuming a max trial length of 2000ms would highly likely yield exclusion based on 
  #' the max rt * 2.5 SD (but that is not a given). 
  #' 
  #' The simplest solution is to exclude these trials as missing.
  #' Importantly, this is not preregistered. 

sum(d$retain_trials == 0) -> loss[["rt_sd_trials"]]
sum(d$retain_trials == 0) / loss$data_trials * 100 -> loss[["rt_sd_pct"]]

d |> 
  filter( retain_trials == 1 ) -> d
```

Furthermore, `r loss[["rt_sd_trials"]]` trials were lost due
to deviating (2.5 SD) response times and none response(s).
Representing a loss of `r round(loss[["rt_sd_pct"]], 2)`
percent of the data.

## corr induc

```{r only correct inducers}
#| include: FALSE
d |>
  mutate( valid_trials = case_when( trial_info=="Inducer trial" & correct_response==1 ~ 1,
                                    trial_info=="Inducer trial" & correct_response==0 ~ 0,
                                    trial_info=="Inducer trial" & is.na(correct_response) ~ 0,
                                      # non-responses count as a wrong response
                                    T ~ NA ) ) |>
  fill(valid_trials, .direction = "up") -> d

sum(d$valid_trials==0) -> loss[["inducer_fail_trials"]]
sum(d$valid_trials==0) / loss$data_trials * 100 -> loss[["inducer_fail_pct"]]

d |> filter( valid_trials == 1 ) -> d
```

Lastly, `r round(loss[["inducer_fail_trials"]], 2)` trials
were removed due to a wrong response on the inducer trial.
Representing a loss of
`r round(loss[["inducer_fail_pct"]], 2)` percent of the data.

A total of
`r loss$exclude_par_trials + loss$rt_sd_trials + loss$inducer_fail_trials`
trials were lost. Representing a loss of
`r round(loss$exclude_par_pct + loss$rt_sd_pct + loss$inducer_fail_pct, 2)`
percent of the data.

## d2: Exclu data

```{r}
#| include: FALSE
# data summary
d |> ungroup() |>
  filter(trial_info=="Diagnostic trial") |>
  group_by(id, congruent) |>
  summarize(rt = mean(rt),
            pct = mean(correct_response)) |>
    #' na.rm is not necessary, because problems related to missing trials 
    #' should be handled by the above criteria.
  pivot_wider( names_from = congruent, values_from = c(rt, pct) ) |>
  ungroup() -> d2
```


# Recalculation of congruency
```{r recalculation of congruency}
# recalculation of the congruency.
colnames(data)

data |>
  group_by(id, inducer_run) |> 
  reframe(stimulus[trial_info=="Inducer instructions"] )

raw_d <- 
  data |> 
  filter(!(id %in% loss$exclude_par)) |>
  select(id, trial_info, diagnostic_run, italic, inducer_run, stimulus, congruent, correct_diag_response_side, response, correct_response,rt) |>
  filter(trial_info=="Diagnostic trial" | trial_info=="Inducer instructions" | trial_info =="Diagnostic instructions") |>
  mutate(inducer_run = as.numeric(inducer_run),
         italic = ifelse(italic=="true", TRUE, FALSE)) |>
  fill(inducer_run, .direction = "up") |>
  # fill(diagnostic_run, .direction="up") #|>
  group_by(id) |>
  mutate( 
    diag_ins = stimulus[trial_info=="Diagnostic instructions"],
    if_italic = str_split(diag_ins, " ") |> map_chr(4) ) |> 
  ungroup() |>
  group_by(id, inducer_run) |> 
  # reframe(ins = stimulus[trial_info=="Inducer instructions"] ) 
  mutate(
    response_side = ifelse(response=="j", "RIGHT", "LEFT"),
    #
    indu_ins = stimulus[trial_info=="Inducer instructions"], 
    ins_left = str_split(indu_ins, " | ") |> map_chr(2),
    ins_right = str_split(indu_ins, " | ") |> map_chr(7),
    ins_resp = str_split(indu_ins, " | ") |> map_chr(4),
    #
    inducer_tap  = ifelse(stimulus==ins_left, ins_resp, ifelse(ins_resp=="LEFT", "RIGHT", "LEFT")),
    diag_tap     = ifelse(italic, if_italic, ifelse(if_italic=="LEFT", "RIGHT","LEFT") ) ) |>
  filter(trial_info=="Diagnostic trial") |> 
  ungroup() |>
  mutate(con = ifelse(inducer_tap==diag_tap, T,F))
```

```{r}
raw_d |> 
  filter(inducer_run>0) |>
  ggplot(aes(con, rt, col=id))+
  #facet_wrap(~id)+
  stat_summary(position=position_dodge(.2))+
  stat_summary(aes(col=NULL), position=position_dodge(.2))
```

```{r}
data |>
  filter(!(id %in% loss$exclude_par)) |>
  select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, congruent) |>
  filter(trial_info == "Diagnostic trial" ) |>#| trial_info == "Inducer trial"
  # Filter the first inducer run (combined practice)
  filter(inducer_run != 0) |> 
  ungroup() |>
  ggplot(aes(congruent, rt, col=id))+
  stat_summary(position=position_dodge(.2))+
  stat_summary(aes(col=NULL), position=position_dodge(.2))
```
# !!!   problem solved    !!! 
```{r}
!raw_d$congruent==raw_d$con
  # !! SOME ARE NOT SIMILAR !! 
raw_d[!raw_d$congruent==raw_d$con,]
```


```{r}
raw_d |> 
  filter(inducer_run>0) |>
  ungroup() |>
  group_by(id, con) |>
  summarise(
    rt = mean(rt,na.rm=T),
    pct=mean(correct_response, na.rm=T)
  ) |> 
  ungroup() -> raw_d_test

raw_d_test |>  
  ggplot(aes(con, rt))+
  stat_summary()
  geom_point()

raw_d_test |> 
  pivot_wider(names_from=con, values_from=c(rt,pct)) |>
  summarise(
    
    rest = t.test(rt_FALSE, rt_TRUE, paired=T)$estimate,
    rt = t.test(rt_FALSE, rt_TRUE, paired=T)$statistic,
    rp = t.test(rt_FALSE, rt_TRUE, paired=T)$p.value,
    pest = t.test(pct_FALSE, pct_TRUE, paired=T)$estimate,
    pt = t.test(pct_FALSE, pct_TRUE, paired=T)$statistic,
    pp = t.test(pct_FALSE, pct_TRUE, paired=T)$p.value,
    #t = t.test(rt_FALSE, rt_TRUE, paired=T)$statistic,
  )

# Without 2.5 SD & wrong inducers, we find a congruency effect. 

```




## Previous tests: 

```{r re-calculated congruency corresponds to filtered task coded congruency }
# Combine the set of data

# Of the filtered data ...
d |>
  mutate(inducer_run = as.numeric(inducer_run)) |>
  left_join(raw_d, by =c("id", "inducer_run", "diagnostic_run")) |>
  select(congruent.x, congruent.y) -> testing

# ... "congruent" corresponds
testing |> 
  mutate(
    c.x = case_when(congruent.x == T ~ 2,
                         congruent.x == F ~ 1,
                         is.na(congruent.x) ~ 0,),
    c.y = case_when(congruent.y == T ~ 2,
                         congruent.y == F ~ 1,
                         is.na(congruent.y) ~ 0, ),
    corr = c.x==c.y ) -> testing

unique(testing$corr)
  #  "congruent" corresponds
```


```{r re-calculated congruency corresponds to raw task coded congruency}
# Of all the data....
data |>
  filter(!(id %in% loss$exclude_par)) |>
  select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, congruent) |>
  filter(trial_info == "Diagnostic trial") |> 
  mutate(inducer_run = as.numeric(inducer_run)) |>
  left_join(raw_d |> select(id, inducer_run, diagnostic_run, congruent), 
            by=c("id", "inducer_run", "diagnostic_run")) |> 
  mutate(c = congruent.x == congruent.y) |>
  pull(c) |>
  unique()
  # ... everything "congruent" corresponds
```

```{r recalc and raw task coded plots correspond}
# Plots the same 
data |>
  filter(!(id %in% loss$exclude_par)) |>
  select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, congruent) |>
  filter(trial_info == "Diagnostic trial") |> 
  mutate(inducer_run=as.numeric(inducer_run)) |>
  merge(raw_d, by=c("id", "inducer_run", "diagnostic_run")) |>
  mutate(c = congruent.x==congruent.y) -> test
  # pull(c) |>
  # unique() # same

test |> 
  pivot_longer(c(congruent.x, congruent.y)) |>
  rename(cong_cond = name, cong_val = value ) |>
  pivot_longer(c(rt.x,rt.y, correct_response.x, correct_response.y)) |>
  ggplot(aes(cong_cond, value))+
  facet_wrap(~name, scales ="free")+
  stat_summary()
```

```{r}
test |> 
  group_by(id, congruent.y) |> 
  summarise(rt=mean(rt.x, na.rm=T),
            pct=mean(correct_response.x,na.rm=T)) -> retest 
retest |>
  pivot_wider(names_from=congruent.y, values_from=c(rt,pct)) -> retest2

t.test(retest2$rt_FALSE, retest2$rt_TRUE, paired=T)
t.test(retest2$pct_FALSE, retest2$pct_TRUE, paired=T)

retest |>
   ggplot(aes(congruent.y, rt)) +
  stat_summary()
```


```{r summarise recalculation and test}
# First test diag_tap/inducer_tap groupby
raw_d |> 
  ungroup() |>
  group_by(id, diag_tap, inducer_tap) |> 
  summarize(
    rt  = mean(rt, na.rm=T), 
    pct = mean(correct_response, na.rm=T)) #|> 
  ungroup() |>
  mutate(con = diag_tap==inducer_tap) |>
  #' THEN combine to congruent
  group_by(id, con) |>
  # recalc
  summarise(
    rt  = mean(rt),
    pct = mean(pct)
  ) |> ungroup() -> sum_d


# simply grouped by congruent
raw_d |>
  ungroup() |>
  group_by(id, congruent) |> 
  summarise(
    rt  = mean(rt, na.rm=T), 
    pct = mean(correct_response, na.rm=T)) -> sum_d2

# These are the same. 
sum_d |>
  left_join(sum_d2, by=join_by("id", "con"=="congruent")) |>
  mutate(rt = rt.x-rt.y,
         pct= pct.x-pct.y)
  
# So why are they different? 
```


```{r test recalculation}

sum_d |> 
  pivot_wider(names_from=con, values_from=c(rt,pct)) |>
  ungroup() ->
    # why is this needed? 
  re_stat_t
t.test(re_stat_t$rt_FALSE, re_stat_t$rt_TRUE, paired=T)
t.test(re_stat_t$pct_FALSE, re_stat_t$pct_TRUE, paired=T)

sum_d2 |>
  pivot_wider(names_from=congruent, values_from=c(rt,pct)) ->
  re_stat_t2
t.test(re_stat_t2$rt_FALSE, re_stat_t2$rt_TRUE, paired=T)
t.test(re_stat_t2$pct_FALSE, re_stat_t2$pct_TRUE, paired=T)
  # `????????????????????????????????????????????????????????????????????

# base
aov(rt ~ diag_tap*inducer_tap + Error( id/(diag_tap*inducer_tap) ), sum_d) |>
  summary()


library(ez)
ez::ezANOVA(sum_d, dv=rt, wid=id, within = con)
ez::ezANOVA(sum_d, dv=pct, wid=id, within = con)

```


```{r}
raw_d |> filter(inducer_run>0) -> d
raw_d |> 
  group_by(id, inducer_run, congruent) |>
  summarise(rt = mean(rt, na.rm=T),
            pct= mean(correct_response, na.rm=T)) |> 
  group_by(id, congruent) |>
  summarise(rt=mean(rt), pct=mean(pct)) |>
  pivot_wider(names_from=congruent, values_from=c(rt,pct)) -> test

t.test(re_stat_t$rt_FALSE, re_stat_t$rt_TRUE, paired=T)
t.test(re_stat_t$pct_FALSE, re_stat_t$pct_TRUE, paired=T)

```




```{r}
sum_d |>
  ungroup() |>
  group_by(diag_tap, inducer_tap) |>
  summarise(
    rt_m   = mean(rt),
    rt_sd  = sd(rt),
    pct_m  = mean(pct),
    pct_sd = sd(pct))

sum_d |>
  ggplot(aes(diag_tap, pct, fill=inducer_tap))+
  stat_summary(aes(col=inducer_tap),geom="col", position=position_dodge(1))+
  stat_summary(geom="errorbar", position=position_dodge(1), width=.4)+
  scale_y_continuous(breaks=seq(0,1,.05))+
  coord_cartesian(ylim=c(0.8, 1))+
  theme(legend.position = "none") -> p1

sum_d |>
  ggplot(aes(diag_tap, rt, fill=inducer_tap))+
  stat_summary(aes(col=inducer_tap),geom="col", position=position_dodge(1))+
  stat_summary(geom="errorbar", position=position_dodge(1), width=.4)+
  scale_y_continuous(breaks=seq(0,1000,50))+
  coord_cartesian(ylim=c(600,750)) -> p2
#-> p2

p1+p2
```


```{r}
test |> 
  mutate(congruent = ifelse(diag_tap==inducer_tap, "congruent","Incongruent"),
        # congruent = factor(congruent, levels=c("Incongruent", "congruent"))
         ) |>
  ggplot(aes(congruent, rt, fill=congruent)) +
  stat_summary(geom="col")+
  stat_summary(aes(group=id), position=position_dodge(.4), geom="point", alpha=.2)+
  stat_summary(aes(group=id), position=position_dodge(.4), geom="line", alpha = .2)+
  coord_cartesian(ylim=c(500,1000))
```

```{r}
data |>
  filter(!(id %in% loss$exclude_par)) |>
  select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, congruent) |>
  filter(trial_info == "Diagnostic trial") |>
  mutate(con = ifelse(congruent==T, "Con", "Incon")) |>
  ggplot(aes(con, rt, #correct_response,
             fill=congruent))+
  stat_summary(geom="col")+
  stat_summary(aes(group=id), position=position_dodge(.2), geom="point", alpha=.2)+
  stat_summary(aes(group=id), position=position_dodge(.2), geom="line", alpha=.2)+
  stat_summary(geom="errorbar", width=.4)+
  scale_y_continuous(breaks=seq(0,2000,100))+
  coord_cartesian(ylim=c(500,1000))
  coord_cartesian(ylim=c(.85,.95))
```





```{r}
raw_d |> 
  mutate(congruent = case_when(diag_tap==inducer_tap ~ "congruent",
                               T ~ "Incongruent")) -> test

test |> 
  filter(!inducer_run==0) |>
  group_by(id,  congruent) |>
  summarize(rt = mean(rt,na.rm=T), 
            pct= mean(correct_response,na.rm=T)) -> test2
test2 |> 
  pivot_wider(names_from=congruent, values_from=c(rt,pct)) -> test3

t.test(test3$rt_Incongruent, test3$rt_congruent, paired=T)
t.test(test3$pct_Incongruent, test3$pct_congruent, paired=T)

extractBF( ttestBF(test3$rt_Incongruent, test3$rt_congruent, paired=T)  )
extractBF( ttestBF(test3$pct_Incongruent, test3$pct_congruent, paired=T)  )

test2 |> 
  ggplot(aes(x=congruent, rt, fill=congruent))+
  stat_summary(geom="col")+
  stat_summary(aes(group=id), position=position_dodge(.4))+
  stat_summary(aes(group=id), position=position_dodge(.4), geom="line")+
  coord_cartesian(ylim=c(500,1000))
  
test2 |> 
  ggplot(aes(x=congruent, pct, fill=congruent))+
  stat_summary(geom="col")+
  stat_summary(aes(group=id), position=position_dodge(.4), alpha=.2)+
  stat_summary(aes(group=id), position=position_dodge(.4), geom="line", alpha = .2)+
  coord_cartesian(ylim=c(0.75,1))

test |> 
  pivot_wider(names_from=congruent, values_from=c(rt,pct)) #-> test2

t.test(test2$rt_congruent, test2$rt_Incongruent, paired=T)
test2 
```
