  ---
title: "Experiment 2 post-cue sampling"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---

# Sample from data 
```{r}
#| echo: false
load("../data/trans/exp1_data.rdata")

library(tidyverse)
library(lsr)
library(pbapply)
library(afex)

```

```{r}
#| echo: false
d <-
  exp1_d |>
  dplyr::select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, con) |>
  mutate(rt = as.numeric( ifelse(rt=="null", NA, rt)) ) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") |>
  filter(!is.na(correct_response))
```

```{r}
#| echo: false
d |>
  filter(diagnostic_run <5) |>
  dplyr::group_by(id, con) |>
  summarise(rt = mean(rt, na.rm=T),
            pct = mean(correct_response,na.rm=T)) |>
  pivot_longer(c(rt,pct)) |>
  ggplot(aes(con, value))+
  facet_wrap(~name, scales ="free")+
  stat_summary()
```
Congruency present for raw data.


```{r}
#| echo: false
diag_trials <-
  d |>
  filter(diagnostic_run<17) |>
  ungroup()
```


First, I will test how many diagnostic trials we need to find a sig difference, based on the results we already have. 
```{r}
map_df(seq_along(1:16), \(x){
  diag_trials |>
    filter(diagnostic_run<x) |>
    group_by(id, con) |>
    summarise(
      rt = mean(rt),
      pct = mean(correct_response)
    ) |> ungroup() |>
    summarise(
      n = x,
      est = t.test(rt~con, paired=T)$estimate,
      t = t.test(rt ~ con, paired=T)$statistic,
      p = t.test(rt ~ con, paired=T)$p.value,
      df = t.test(rt ~ con, paired=T)$parameter,)
}) -> n_back_test
n_back_test |>
  ggplot(aes(n, p))+
  geom_line()+
  geom_hline(yintercept=.05)
```
Sampling from all blocks, across all participants, we appear to need between 8-10 trials (per block) to find a significant congruency effect. 


Now I will attempt to see how many diagnostic trials are necessary to find the congruency effect through random sampling of the data from experiment 1. 
```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
stat_test <- function(size){
  # n = size 
  
  slice_sample(diag_trials, by = c(id, con), n = size) |>
    group_by(id, con) |>
    summarise(l = length(rt),
              rt = mean(rt),
              pct = mean(correct_response)) |>
    ungroup() |>
    pivot_wider(names_from=con, values_from=c(l, rt,pct)) |>
    reframe(
      n = size,
      rt_con = mean(rt_TRUE, na.rm=T),
      rt_incon = mean(rt_FALSE, na.rm=T),
      rt_est  = mean(rt_FALSE-rt_TRUE, na.rm=T),
      rt_t  = t.test(rt_FALSE, rt_TRUE, paried=T, na.rm=T)$statistic,
      rt_p  = t.test(rt_FALSE, rt_TRUE, paried=T, na.rm=T)$p.value,
      rt_d = cohensD(rt_FALSE, rt_TRUE, method="corrected"),
      pct_con = mean(pct_TRUE, na.rm=T),
      pct_incon = mean(pct_FALSE, na.rm=T),
      pct_est  = mean(pct_FALSE-pct_TRUE, na.rm=T),
      pct_t  = t.test(pct_FALSE, pct_TRUE, paried=T, na.rm=T)$statistic,
      pct_p  = t.test(pct_FALSE, pct_TRUE, paried=T, na.rm=T)$p.value,
      pct_d = cohensD(pct_FALSE, pct_TRUE, method="corrected")
    )
}
map_df(1:50, \(x){
  stat_test(x)
}) -> sample1 

sample1 |>
  ggplot(aes(n, rt_p))+
  geom_line()
```

Generally this appears to be unstable. The increase in diagnostic trials does not appear to change the significance at all. Weird.

To solve this, we can repeat each sampling of a particular size a couple of times to average the differences.
```{r}
#| echo: false
#| error: false
#| warning: false
#| message: false
rep_test <- function(maxdiag, rep){
  # range = number of diagnostics
  # rep = how many times we repeat the same sampling

  map_df(1:maxdiag, \(x){
    # for each diag we repeat the diag length "rep" times
    map_df(1:rep, \(y){
      stat_test(x)
    })
  })
}
rep_test(50, 30) -> sample2
  # From 1 to 50 diagnostic trials, repeat each 20 times. 
sample2 |>
  ggplot(aes(n, rt_p))+
  stat_summary()
  geom_line()
```


```{r}
sample2 |>
  ggplot(aes(n, rt_d))+
  stat_summary()
  geom_line()
```
This did not appear to solve the problem at all. 

I suspect there is some unequalness with the sampling... 
We can move over to generated data to see how increased samples "should" trend the data towards more significant differences. 

# Sampling from generated data 

Generate normal distributions with a mean of 0 and a standard deviation of 1.
Effect.size adjusts the effect on the congruent trials by "eff.size" difference.
For our purposes here "rt" and "pct" can be on the same scale.
```{r}
gen_sample <- function(N, OBS, eff.size){
  #' @param N Number of subjects
  #' @param OBS Number of observation per N
  #' @param eff.size Effect size difference between incongruent and congruent trials

  tibble(
    id = rep(1:N, each=OBS),
    rt_inc  = rnorm(N*OBS, 0, 1),
    rt_con  = rnorm(N*OBS, 0 + eff.size, 1),
    pct_inc = rnorm(N*OBS, 0, 1),
    pct_con = rnorm(N*OBS, 0 + eff.size, 1) )
}
```


We can then generate random data related to each effect size with different diagnostic lengths.
In this case, effect size in steps of .05 from 0 to .5. Moreover, we use 30 to 70 diagnostic trials.
60 trials would correspond to an average of 2.5 trials in each block, and what would correspond to a range of 0 to 5 trials (post-cue trials).

```{r}
#| echo: false
map_df(seq(0,.5, .05), \(y){
  map_df(30:70, \(x){
    gen_sample(27, x, y) |>
      summarise(
        eff = y,
        count = x,
        size = x,
        rt.con = mean(rt_con, na.rm=T),
        rt.incon = mean(rt_inc, na.rm=T),
        rt.est  = mean(rt_inc-rt_con, na.rm=T),
        rt.t  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$statistic,
        rt.p  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$p.value,
        rt.d = cohensD(rt_inc, rt_con, method="corrected"),
        pct.con = mean(pct_con, na.rm=T),
        pct.incon = mean(pct_inc, na.rm=T),
        pct.est  = mean(pct_inc-pct_con, na.rm=T),
        pct.t  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$statistic,
        pct.p  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$p.value,
        pct.d = cohensD(pct_inc, pct_con, method="corrected")
      )
  })
}) -> gen_data

gen_data |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(n, value, col = eff, group=eff ))+
  facet_wrap(~name, scales="free")+
  stat_summary(geom="line")+
  geom_smooth(method="lm")+
  scale_y_log10()+
  geom_hline(yintercept=0.05)+
  geom_vline(xintercept=60)
```

In this graph we see a clear increase in p for increasing diagnostic lengths. 

```{r}
gen_data |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(n, rt.d, col = eff, group=eff ))+
  stat_summary(geom="line")+
  geom_smooth(method="lm", alpha =.2)
  #scale_y_log10()
```

Similarly, the effect size seems to be rather stable for all diagnostic lengths. 


From the generated data, we can calculate the power for each effect size: 

```{r}
#| echo: false

gen_data |>
  group_by(eff) |>
  summarise(power.rt = mean(rt.p<.05), power.pct = mean(pct.p<.05)) |>
  gt()
```


Let us sample multiple times with 60 diagnostic trials:

```{r}
#| echo: false
#| message: false
map_df(seq(0,.5, .05), \(y){
  map_df(1:1000, \(x){
    gen_sample(27, 60, y) |>
      summarise(
        eff = y,
        count = x,
        size = 60, 
        rt.con = mean(rt_con, na.rm=T),
        rt.incon = mean(rt_inc, na.rm=T),
        rt.est  = mean(rt_inc-rt_con, na.rm=T),
        rt.t  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$statistic,
        rt.p  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$p.value,
        rt.d = cohensD(rt_inc, rt_con, method="corrected"),
        pct.con = mean(pct_con, na.rm=T),
        pct.incon = mean(pct_inc, na.rm=T),
        pct.est  = mean(pct_inc-pct_con, na.rm=T),
        pct.t  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$statistic,
        pct.p  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$p.value,
        pct.d = cohensD(pct_inc, pct_con, method="corrected")
      )
  })
}) -> gen_data2


gen_data2 |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(count, value, col = eff, group=eff ))+
  facet_wrap(~name, scales="free")+
  stat_summary(geom="line", alpha = .4)+
  geom_smooth(method="lm")+
  scale_y_log10()+
  geom_hline(yintercept=0.05)
```

This yields a very nicely coloured graph. At any rate, we can see that there is some variation around each effect size, but that it is generally stable over time.

From this, we can calculate the power for each effect size:
```{r}
#| echo: false

gen_data2 |>
  group_by(eff) |>
  summarise(power.rt = mean(rt.p<.05), power.pct = mean(pct.p<.05)) |>
  gt()
```

With an effect size of at least .10, we will get a power of 80%.

In line with our results, we found .17 for rt and .70 with accuracy. 
This means we should be able to detect both response time and accuracy with a high than 98% power. 



Testing slightly less trials, due to loss at the first inducer block: 
```{r}
#| echo: false
#| message: false
map_df(seq(0,.5, .05), \(y){
  map_df(1:1000, \(x){
    gen_sample(27, 55, y) |>
      summarise(
        eff = y,
        count = x,
        size = 55, 
        rt.con = mean(rt_con, na.rm=T),
        rt.incon = mean(rt_inc, na.rm=T),
        rt.est  = mean(rt_inc-rt_con, na.rm=T),
        rt.t  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$statistic,
        rt.p  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$p.value,
        rt.d = cohensD(rt_inc, rt_con, method="corrected"),
        pct.con = mean(pct_con, na.rm=T),
        pct.incon = mean(pct_inc, na.rm=T),
        pct.est  = mean(pct_inc-pct_con, na.rm=T),
        pct.t  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$statistic,
        pct.p  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$p.value,
        pct.d = cohensD(pct_inc, pct_con, method="corrected")
      )
  })
}) -> gen_data3

gen_data3 |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(count, value, col = eff, group=eff ))+
  facet_wrap(~name, scales="free")+
  stat_summary(geom="line", alpha = .4)+
  geom_smooth(method="lm")+
  scale_y_log10()+
  geom_hline(yintercept=0.05)
```

```{r}
#| echo: false

gen_data3 |>
  group_by(eff) |>
  summarise(power.rt = mean(rt.p<.05), power.pct = mean(pct.p<.05)) |>
  gt()
```

Still results in rather similar outcomes. A solid > 98% power at an effect size of .15. 


# Congruency check 
A last notion relates to whether we need to or should force an equal number of diagnostic trials. 

```{r}
d |>
  filter(!is.na(con)) |>
  group_by(id, con) |>
  count() |>
  mutate(trials = n - 30) |>
  ggplot(aes(trials))+
  geom_histogram(alpha = .4)+
  geom_histogram(aes(x=n, fill="all data"), alpha=.4)+
  scale_x_continuous(breaks = seq(5,200, 5))
```

In relation to the simulated data above, removing 30 trials does not lead to problems with regards to finding a congruency effect. 
With at least 75 trials for some conditions. 

The general conclusion from this is that we do not need to force an equal number of right/left diagnostic trials for the **pre-cue**, but to be sure we receive enough trials for the **post-cue** we will force that to be equal. 





# Simulating experiment 2: cong * cue

We can simulate the tests for the experiment by adding an additional factor (cue).
We have two main effects, congruency and cue, including an interaction congruency*cue.
We hypothesize an interaction between the cue and the congruency, while the main effect of congruency will remain weak if existent. 
We do not hypothesize an effect of the cue, but we expect it to interact with the congruency. 
In this way, we expect a congruency effect ONLY for trials AFTER the cue.

In the function below we can generate the experimental setup: 
```{r}
exp2_sim2x2 <- function(N, blocks, block.len, eff.con.pre, eff.con.post, eff.con.cue.intr){
  #' @param N Number of subjects
  #' @param blocks Number of blocks 
  #' @param block.len Length of each block -- this is split, half the trials are pre and half are post
  #' @param eff.con.pre Effect size of the congruency effect
  #' @param eff.cue.post Effect size of the pre-post cue
  #' @param eff.con.cue.intr Effect size of the interaction

  tibble(
    id     = rep(1:N, each = blocks * block.len),
    block  = rep.int( rep(1:blocks, each = block.len), N),
    cue    = rep.int( rep(c("pre", "post"), each = block.len/2), blocks * N),
    con    = rep.int( c("con", "incon"), N * blocks * block.len/2),
    rt     = rnorm(N * blocks * block.len, 
                   0 - as.integer(con=="con") * eff.con.pre + 
                          # Main effect of congruency ! on the pre trials ! 
                       as.integer(cue=="post") * eff.con.post + 
                          # Main effect of "cue"
                          # We do not really believe the cue will influence RT, but that it will interact with congruency
                       -as.integer(con=="con" & cue=="post") * eff.con.cue.intr,
                          # Interaction between congruency and cue
                          # We believe that the post-cue will interact with the congruency
                   1)
  ) -> d
  
  # Here we remove some post trials equal to half 
  # First we take all the pre trials: 
  d |> 
    filter(cue=="pre") |>
  # Then we combine the filtered post trials: 
  rbind(
    d |> 
      # We filter in such a way that we slice the first trials for each id in each trial:
      filter(cue=="post") |>
      group_by(id, block) |>
      slice_head(n = floor( block.len/2/2 ) )
  )
}
```

```{r simulate experiment 2}
#| echo: false

# Create a "cluster" to simulate faster
# Take half the computers cores 
makeCluster( floor(detectCores() - detectCores() * 1/2) ) -> cl
# Export the local function
clusterExport(cl, varlist = c("exp2_sim2x2"))
# Initiate libraries in clusters
clusterEvalQ(cl, {
  library(tidyverse)
  library(pbapply)
  library(afex)
})

# Apply function across "cl"
pblapply(10:100, \(x){
  # apply *x* participants...
  purrr::map_df(1:500, \(y){
    # repeat each par-size 100 times
    exp2_sim2x2(x, 24, 12, .05, .008, .17) -> d
    d |> 
      aov_car(rt ~ con*cue + Error(id/(con*cue)), data = _, fun_aggregate=mean) -> 
      res
    tibble(
      size = x,
      itr = y,
      name = c(names(res$Anova$SSP[2]),names(res$Anova$SSP[3]), names(res$Anova$SSP[4])),
      ges = res[["anova_table"]][["ges"]],
      p = res[["anova_table"]][["Pr(>F)"]],
    ) |> pivot_wider(names_from = name, values_from =c(ges, p))
  }) |> summarise(size = unique(size),
                  itr = max(itr),
                  pow.con = mean(p_con<.05),
                  pow.cue = mean(p_cue<.05),
                  pow.con.cue = mean(`p_con:cue`<.05),
                  across(contains("_"), mean))
}, cl = cl) |>
  map_df(~.x) -> eachPar500

# stop cluster
stopCluster(cl)
```

```{r}
#| echo: false
eachPar500 |> 
  pivot_longer(c(p_con, p_cue, `p_con:cue`)) |>
  ggplot(aes(size, value, col=name, group=name))+
  geom_line()+
  geom_hline(yintercept=.05)+
  geom_vline(xintercept=27)+
  scale_x_continuous(breaks=seq(0,200,10))+
  annotate("text",x = 37, y=.09, label = "27 participants")+
  labs(y="p value", x="Sample size", col = "Effects")
```

Generally, recruiting around 25 participants seems to suffice to find a significant effect.

```{r}
#| echo: false

eachPar500 |> 
  pivot_longer(c(pow.p, pow.c, pow.p.c)) |>
  mutate(name = str_replace_all(name, "pow.", "")) |>
  ggplot(aes(size, value, col=name)) +
  labs(y="Power", x="Sample size", col="Effect")+
  coord_cartesian(ylim=c(.6, 1), xlim=c(10, 50))+
  scale_x_continuous(breaks=seq(0,200,10))+
  geom_line()+
  geom_hline(yintercept=.95, col ="darkblue", linetype="dashed")+
  geom_vline(xintercept = 30, col = "darkblue", linetype="dashed")+
  annotate("text",x = 35, y=.92, label = "30 participants", col="darkblue")+
  geom_hline(yintercept=.8, col="purple")+
  geom_vline(xintercept = 20, col = "purple")+
  annotate("text",x = 25, y=.77, label = "20 participants", col="purple")
```

As for the power, we can obtain a solid power (95%) for the interaction between cue and congruency with 30 participants.

```{r}
#| echo: false
#| message: false
eachPar500 |> 
  pivot_longer(c(ges_con, ges_cue, `ges_con:cue`)) |> 
  mutate(name = str_replace_all(name, "ges_", "")) |>
  ggplot(aes(size, value, col=name, group=name))+
  geom_line()+
  geom_hline(yintercept=.05)+
  geom_smooth(method="lm",  linewidth=.5, alpha=.2)+
  scale_y_continuous(breaks=seq(0,1,.05))+
  labs(y="ges", x="Sample size")
```

The effect size seems to slightly decrease with increasing sample size.


## Visualize the expected difference
```{r}
#| echo: false
#| message: false

pblapply(1:30, \(i) exp2_sim2x2(30,24,12, .05, .008, .17) |> mutate(v=i) ) |> 
  map_df(~.x) -> par30

par30 |>
  ggplot(aes(con, rt, col=cue, group=interaction(v,cue)))+
  stat_summary(aes(col=NULL), alpha=.2, position=position_dodge(.1))+
  stat_summary(aes(col=NULL), alpha=.2, geom="line", position=position_dodge(.1))+
  stat_summary(aes(group=cue), geom="line", linewidth=1.5, position=position_dodge(.1))+
  stat_summary(aes(group=cue), linewidth=1.5, position=position_dodge(.1))
```
The plot illustrates the expected interaction simulated 30 times. 


