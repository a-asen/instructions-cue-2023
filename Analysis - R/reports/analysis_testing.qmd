---
title: "exp1_results"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---

```{r libraries }
#| include: FALSE
library(tidyverse)
library(ggpp)
library(gt)
library(patchwork)
```

```{r load data }
#| include: FALSE
list.files("../data/raw/pilot", pattern = "*.csv", full.names = T) -> fnames

fnames2 <- fnames[str_detect(fnames, "2024")][1:4]
map_df(fnames2, \(x){
  read_csv(x)
}) -> data
```


```{r transformation}
#| include: FALSE
# Pre-transformation
data |>
  mutate(rt = as.integer(ifelse( rt == "null", NA, rt ))) -> data

# Select the relevant columns & rows
data |>
  select(id, trial_info, inducer_run, correct_response, rt, congruent) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") -> d

# d |> filter(!inducer_run=="practice" ) -> d
  # pilot removing
```


```{r exclusion start - accuracy}
#| include: FALSE
loss <- list()
loss$data_trials <- nrow(d)

# d |> filter(inducer_run > 0) -> d
  # Pilot removing

### Overall accuracy      ====
d |>
  filter( trial_info == "Diagnostic trial" | trial_info == "Inducer trial" ) |>
  group_by( id ) |>
  summarize( acc = sum(correct_response, na.rm = TRUE) / length( !is.na(correct_response) ) ) |>
  filter( acc < .7 ) |>
  pull( id ) -> loss$exclude_par

loss[["exclude_par_trials"]] <- length( d$rt[d$id == loss$exclude_par] )
loss[["exclude_par_pct"]] <- length( d$rt[d$id == loss$exclude_par] ) / loss$data_trials * 100

d |> filter( !(id %in% loss$exclude_par) ) -> d
``` 

In total, `r length(loss[["exclude_par"]])` `r ifelse(length(loss[["data_trials"]]) > 1, "participants", "participant")` where excluded due to low accuracy (> 70%). Resulting in a loss of `r loss[["exclude_par_pct"]]` percent of the data.

```{r due to high SD & NA responses }
#| include: FALSE
# Removing trials more than 2.5 SD (from individual mean) & NA RT 
d |>
  group_by(id) |>
  mutate(rt_crit = ifelse( trial_info == "Diagnostic trial",
                           mean( rt, na.rm = TRUE ) + sd( rt, na.rm = TRUE ) * 2.5,
                           NA ),
         retain_trials = ifelse(
           # Remove deviations more than 2.5 SD
           rt >= rt_crit & trial_info == "Diagnostic trial" |
             # AND remove slow responses
             is.na(rt) & trial_info == "Diagnostic trial",
                             0, 1 ) ) -> d

sum(d$retain_trials == 0) -> loss[["rt_sd_trials"]]
sum(d$retain_trials == 0) / loss$data_trials * 100 -> loss[["rt_sd_pct"]]

d |> 
  filter( retain_trials == 1 ) -> d
```

Furthermore, `r loss[["rt_sd_trials"]]` trials were lost due to deviating (2.5 SD) response times and none response(s). Representing a loss of `r round(loss[["rt_sd_pct"]], 2)` percent of the data. 

```{r only correct inducers}
#| include: FALSE
d |>
  mutate( valid_trials = case_when( trial_info=="Inducer trial" & correct_response==1 ~ 1,
                                    trial_info=="Inducer trial" & correct_response==0 ~ 0,
                                    T ~ NA ) ) |>
  fill(valid_trials, .direction = "up") -> d

sum(d$valid_trials==0) -> loss[["inducer_fail_trials"]]
sum(d$valid_trials==0) / loss$data_trials * 100 -> loss[["inducer_fail_pct"]]
loss
d |> filter( valid_trials == 1 ) -> d
```

Lastly, `r round(loss[["inducer_fail_trials"]], 2)` trials were removed due to a wrong response on the inducer trial. Representing a loss of `r round(loss[["inducer_fail_pct"]], 2)` percent of the data. 

A total of `r loss$exclude_par_trials + loss$rt_sd_trials + loss$inducer_fail_trials` trials were lost. Representing a loss of `r round(loss$exclude_par_pct + loss$rt_sd_pct + loss$inducer_fail_pct, 2)` percent of the data. 


```{r}
#| include: FALSE
# data summary
d |>
  filter(trial_info=="Diagnostic trial") |>
  group_by(id, congruent) |>
  summarize(rt = mean(rt, na.rm = TRUE),
            pct = sum(correct_response==1) / length(correct_response)) |>
  pivot_wider( names_from = congruent, values_from = c(rt, pct) ) |>
  ungroup() -> d2
```

```{r RT tables}
#| echo: false

# Freq:
## response times
rt_test <- t.test( d2$rt_FALSE, d2$rt_TRUE, paired = TRUE, alternative = "greater" )
d2 |>
  summarise(
    name = "RT",
    m_incongruent = mean(rt_FALSE),
    sd_incongruent = sd(rt_FALSE),
    m_congruent = mean(rt_TRUE),
    sd_congruent = sd(rt_TRUE),
    Mdiff = mean( rt_FALSE - rt_TRUE),
    t = rt_test$statistic,
    df = rt_test$parameter,
    p = rt_test$p.value,
    conf.l = rt_test$conf.int[1],
    conf.h = rt_test$conf.int[2],
    p.cor = p.adjust(p, "bonferroni", n=2),
    d = Mdiff / sqrt(
      ( (length(rt_FALSE)-1) * sd_incongruent^2 + (length(rt_TRUE)-1) * sd_congruent^2 ) / 
        ( length(rt_FALSE) + length(rt_TRUE) - 2 )),
    )  -> d_rt

## proportion correct trials
pct_test <- t.test(  d2$pct_FALSE, d2$pct_TRUE, paired = TRUE, alternative = "less" )
d2 |>
  summarise(
    name = "PCT",
    m_incongruent = mean( pct_FALSE ),
    sd_incongruent = sd( pct_FALSE ),
    m_congruent = mean( pct_TRUE ),
    sd_congruent = mean( pct_TRUE ),
    Mdiff = mean( pct_FALSE - pct_TRUE ),
    t = pct_test$statistic,
    df = pct_test$parameter,
    p = pct_test$p.value,
    p.cor = p.adjust(p, "bonferroni", n=2),
    conf.l = pct_test$conf.int[1],
    conf.h = pct_test$conf.int[2],
    d = Mdiff / sqrt(
      ( (length(rt_FALSE)-1) * sd_incongruent^2 + (length(rt_TRUE)-1) * sd_congruent^2 ) / 
        ( length(rt_FALSE) + length(rt_TRUE) - 2 )),
  ) -> d_pct

#https://chat.openai.com/c/2f8ccce2-3396-4f85-b17a-d73fdaf4b34e


library(BayesFactor)
BayesFactor
bayestestR::hdi(test) -> test2

test2$CI_high

test@
test2[1,c("CI_low", "CI_high")]

as.data.frame(test[,1:4]) |>
  summarize(t = mean(mu))

sloop::otype(test2)
test$conf.int
test$stderr
test$estimate
# bayesian

BayesFactor::ttestBF(d2$rt_FALSE, d2$rt_TRUE, paired = T, iterations = 10000, posterior = T) -> test
mean(test)
mean(test[,"mu"])
# S4 object has @ access (instead of $)
test@bayesFactor
test@numerator

```


```{r stat table}
#| echo: false
rbind(d_rt, d_pct) |> summarize(x=sum(df)/length(df)) |> pull() -> d2_df
if(d2_df != floor(d2_df)){warning("NOT SIMILAR, CHECK DF")}
  # degrees of freedom

rbind(d_rt, d_pct) |>
  # add bayes row? 
  mutate(ps = case_when(p.cor < .05 ~ "*", p < 0.01 ~ "**", p < 0.001 ~ "***", T ~ ""),
         em1 = "", em2="") |>
  gt() |>
  tab_spanner(  "Incongruent", c(m_incongruent, sd_incongruent) ) |>
  cols_label(   m_incongruent = "M", sd_incongruent = "SD" ) |>
  tab_spanner(  "Congruent", c(m_congruent, sd_congruent) ) |>
  cols_label(   m_congruent = "M",  sd_congruent = "SD" ) |>
  ##
  cols_move(    "ps", Mdiff ) |> 
  cols_move(    "em1", sd_incongruent ) |>
  cols_move(    "em2", ps ) |>
  tab_spanner(  "Interval", c(conf.l, conf.h)) |> 
  cols_label(   conf.l="Low", conf.h="High") |>
  cols_hide(    c(p, t, df, p.cor) ) |> 
  cols_label(   em1="",em2="", ) |>
  cols_align(   "center", c(2:6, conf.l, conf.h)) |> 
  cols_align(   "left", ps) |> 
  cols_label(   ps = "" )  |>
  #fmt_markdown() |>
    # Doesnt play nice with .docx (?)
  fmt_number()

  # These do not work, for some reason.
  # tab_footnote( "*p < 0.05, **p < 0.01, ***p < 0.001" ) |>
  # tab_footnote( "P's are Bonferroni corrected for 2 tests", placement="left" ) |>
```
`r cat("*p < .05, **p < .01, ***p < .001")`
M = Mean, SD = standard deviation, df = `r  d2_df``.
*Note.* P's are Bonferroni corrected for 2 tests.



```{r rt and percent plot}
#| echo: false
#| fig-dpi: 300

# d2 |>
#   mutate(congruent = ifelse(str_detect(congruent, "TRUE"), "Congruent", "Incongruent")) |>
#   ggplot(aes(x = congruent, y = rt))+
#   geom_point(position = position_dodgenudge(.05, x = .05), alpha = .35)+
#   geom_line(aes(group = id), position = position_dodgenudge(.05, x=.05), alpha = .4)+
#   stat_summary(fun.data = mean_se, col = "red")+
#   labs(x = "", y = "Response time (ms)") -> p1
# 
# d2 |>
#   mutate(congruent = ifelse(str_detect(congruent, "TRUE"), "Congruent", "Incongruent")) |>
#   ggplot(aes(x = congruent, y = pct))+
#   geom_point(position = position_dodgenudge(.05, x = .05), alpha = .35)+
#   geom_line(aes(group = id), position = position_dodgenudge(.05, x=.05), alpha = .4)+
#   stat_summary(fun.data = mean_se, col = "red")+
#   scale_y_continuous(breaks = seq(0,1,.05), limits = c(.68,1))+
#   labs(x = "", y = "Proportion of correct ") -> p2
# 
# p1+p2
  # Visualize the difference, not the sum (paired test). 

d2 |>
  mutate(rt_diff = rt_FALSE-rt_TRUE,
         pct_diff = pct_FALSE-pct_TRUE) -> d3

d3 |>
  mutate(across( where(is.numeric), scale)) |>
    # normalize
  ggplot(aes(rt_diff))+
  stat_function(aes(x,), tibble(x=c(-3,3)), fun=dnorm, col="red", linewidth=2, alpha=.3)+
  geom_density()+
  scale_x_continuous(breaks=seq(-3,3,1))+
  labs(x="Standardized difference") -> p1
    # Theoretical normal distribution over 0

d3 |>
  mutate(across( where(is.numeric), scale)) |>
    # normalize
  ggplot(aes(pct_diff))+
  stat_function(aes(x,), tibble(x=c(-3,3)), fun=dnorm, col="red", linewidth=2, alpha=.3)+
  geom_density()+
  scale_x_continuous(breaks=seq(-3,3,1))+
  labs(x="Standardized difference", y="") -> p2
    # Theoretical normal distribution over 0

p1+p2
```



