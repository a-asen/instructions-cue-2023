---
title: "How many diagnostic pairs do we need?"
subtitle: "Slicing from experiment 1"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---

# Start

```{r params}
#| include: false

run_simulations <- FALSE
save_simulation_data <- FALSE
load_simulation_data <- TRUE
```

```{r libraries}
#| include: false

library(tidyverse)
library(arrow)     # load parquet files 
library(parallel)  # Parallel processing
library(pbapply)   # Progression bar + parallel processing
library(broom)

# ? 
# library(lsr)
# library(afex)
```

```{r create cluster}
#| include: false
if(run_simulations){
  cl <- makeCluster(detectCores()*.95)
  clusterEvalQ(cl, {
    library(tidyverse)
    library(tidymodels)
  })
  clusterExport(cl, "d_ex1")
} else { cl <- NULL }
```

# data

```{r load data}
#| include: false
list.files("../data/raw/experiment1/task", pattern = "*.csv", full.names = T) -> fnames1

map_df(fnames1, \(x){
  read_csv(x)
}) -> exp1_d

load("../data/simulations/diag_simu_sum.rdata")

if(load_simulation_data){
  # Random slicing: 
  ## Random slicing of the 27 participants: 
  rnd_slice_diag_exclu <- 
    read_parquet("../data/simulations/exp2_diagnostic_length_sampling_from_exp1_raw.parquet")
  sum_rnd_slice_diag_exclu <-
    read_parquet("../data/simulations/exp2_diagnostic_length_sampling_from_exp1_sum.parquet")
  ## Random slicing over increasing sample size (fake ids)
  rnd_extended_slice_diag_exclu <- 
    read_parquet("../data/simulations/exp1_rnd_sampling_inc_samp_size.parquet")
  sum_rnd_extended_slice_diag_exclu <-
    read_parquet("../data/simulations/exp1_sum_rnd_sampling_inc_samp_size.parquet")
 
  ## Random slicing over increasing sample size and integrated score calculations
  rnd_int_scores_extended_slice_diag_exclu <- 
    read_parquet( "../data/simulations/exp1_rnd_sampling_inc_samp_size_integrated_scores.parquet")
  sum_rnd_int_scores_extended_slice_diag_exclu <- 
    read_parquet( "../data/simulations/exp1_sum_rnd_sampling_inc_samp_size_integrated_scores.parquet") 
  #    write_parquet(diag_lens_over_samp_sizes, "../data/simulations/diag_lens_over_sample_sizes.parquet")
    #write_parquet(sum_diag_lens_over_samp_sizes, "../data/simulations/diag_lens_over_sample_sizes_sum.parquet")
}
```

```{r preprocess}
#| include: false
# preprocess 
source("rep_munge/exp1_preprocess.R")
```

# Experiment 1:

## Estimating the expected loss of data (trials)

```{r}
#| include: false
d_ex1 |>
  filter(trial_info=="Diagnostic trial") |>
  group_by(id) |>
  count() |> 
  pull(n) |>
  mean() -> exp1_remaining_trials
```

After the exclusion criteria, we are left with
`r exp1_remaining_trials` trials.

This corresponds to a loss of `r exp1_remaining_trials/240`
(or `r exp1_remaining_trials/240*100` percent). 

In other words, whatever post-cue run we decide, we need to
estimate an expected loss of about `r exp1_remaining_trials/240`.


## Experiment 1 outcome values

### t.test

Calculate the standardized means and standard deviations, and
effect sizes from experiment 1.

(note. this is for the *excluded* data)

```{r exp1 outcomes}
#| echo: false

d_ex1 |> 
  ungroup() |>
  filter(!is.na(con)) |>
  summarise(
    rt = mean(rt), 
    pe = 1 - mean(correct_response),
    .by = c(id, con)
  ) |> 
  mutate(
    rt = scale(rt),
    pe = scale(pe)
  ) |>
  pivot_wider(names_from = con, values_from = c(rt,pe)) |>
  summarise(
    rt_d_m = mean(rt_FALSE-rt_TRUE),
    rt_d_sd = sd(rt_FALSE-rt_TRUE),
    rt_d = rt_d_m / rt_d_sd,
    pe_d_m = mean(pe_FALSE-pe_TRUE),
    pe_d_sd = sd(pe_FALSE-pe_TRUE),
    pe_d = pe_d_m / pe_d_sd,
  ) -> exp1_outcome
exp1_outcome
```

# Slicing from experiment 1:
Parameters:

```{r}
n_diag_pairs <- 36 # number of diagnostic pairs
loss_trials  <- exp1_remaining_trials/240
loss_trials 
```

We determine to use around 36 pairs (72 trials) for the post-cue run, corresponding to an average of 3 trials per post-cue run. These runs are displayed as the black dashed line, and the expected remaining trials (after loss) are indicated with a dashed red line. 

## Slice from the START

```{r}
#| echo: false
pblapply(2:90, \(pairs){
  d_ex1 |>
    filter(inducer_run>0) |>
    slice_head(n = pairs, by = c(id, con)) |>
    summarise(
      .by = c(id, con),
      rt = mean(rt),
      pe = 1 - mean(correct_response)
    ) -> test
  
  test |>
    t.test(rt ~ con, data=_, paired = T) |> 
    tidy() |> mutate(.before=1, n = "rt") |>
    rbind(
      test |> 
        t.test(pe ~ con, data=_, paired=T) |> 
        tidy() |> mutate(.before=1, n="pe")
    ) |> 
    mutate(.before=1, pairs=pairs)
}, cl = cl) |> 
  map_df(~.x) -> 
  head_slice_diag_exclu
```

```{r slice from start}
#| echo: false
head_slice_diag_exclu |>
  mutate(
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  #pivot_wider(names_from=n, values_from = 3:10) |>
  ggplot(aes(pairs, p.value, col = n)) + 
  geom_line()+
  geom_hline(yintercept=.05)+
  geom_vline(xintercept=n_diag_pairs, linetype="dashed")+
  geom_vline(xintercept=n_diag_pairs*loss_trials, linetype="dashed", col="red") +
  labs(x = "Diagnostic pairs", y = "p value", col = "Variable") +
  theme_bw() +
  theme(legend.position = "none")
```

Slicing from the **start** (!) suggest that we need around 20
pairs (i.e., 40 trials) to a achieve a significance result.

## Slice from the END

```{r slice from end}
#| include: false
pblapply(2:90, \(pairs){
  d_ex1 |>
    filter(inducer_run>0) |>
    slice_tail(n = pairs, by = c(id, con)) |>
    summarise(
      .by = c(id, con),
      rt = mean(rt),
      pe = 1 - mean(correct_response)
    ) -> test
  
  test |>
    t.test(rt ~ con, data=_, paired = T) |> 
    tidy() |> mutate(.before=1, n = "rt") |>
    rbind(
      test |> 
        t.test(pe ~ con, data=_, paired=T) |> 
        tidy() |> mutate(.before=1, n="pe")
    ) |> 
    mutate(.before=1, pairs=pairs)
}, cl = cl) |> 
  map_df(~.x) -> 
  tail_slice_diag_exclu
```

```{r end slice vis }
#| echo: false
tail_slice_diag_exclu |>
  mutate(
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  #pivot_wider(names_from=n, values_from = 3:10) |>
  ggplot(aes(pairs, p.value, col = n)) + 
  geom_line() +
  geom_hline(yintercept=.05) +
  geom_vline(xintercept=n_diag_pairs, linetype="dashed") +
  geom_vline(xintercept=n_diag_pairs*loss_trials, linetype="dashed", col="red") + 
  labs(x = "Diagnostic pairs", y = "p value", col = "Variable") +
  theme_bw() +
  theme(legend.position = "none") -> p1
p1
```

Slicing from the **end** reveal a similar pattern, but would
require more trials - around 25 trials for PE, but even more
for RT.

```{r legends for e-s, echo=FALSE}
tail_slice_diag_exclu |>
  mutate(
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  #pivot_wider(names_from=n, values_from = 3:10) |>
  ggplot(aes(pairs, p.value, col = n)) + 
  labs(col="Variable") +
  theme_bw() + 
  theme(legend.position = "top") +
  geom_line() -> p1
ggpubr::as_ggplot(ggpubr::get_legend(p1))
```


## Randomly slicing

Instead of slicing from the start/end, we can randomly slice.

```{r rnd sampling}
#| echo: false
if(run_simulations){
  pblapply(2:100, \(pairs){
    map_df(1:900, \(itr){
      d_ex1 |>
        filter(!is.na(con)) |>
        slice_sample(n = pairs, by = c(id, con)) |>
        summarise(
          .by = c(id, con),
          rt_sd = sd(rt),
          rt = mean(rt),
          pe_sd = sd(correct_response),
          pe = 1 - mean(correct_response)
        ) -> test
      
      test |>
        select(-rt_sd, -pe_sd) |>
        mutate(rt = scale(rt), pe = scale(pe)) |>
        pivot_longer(c(rt,pe)) |>
        pivot_wider(names_from = con, values_from = value) |>
        summarise(
          .by = name,
          m_d   = mean(`FALSE` - `TRUE`),
          sd_d  =   sd(`FALSE` - `TRUE`) ) |>
        left_join(
          by = join_by(name == n),
          test |>
            t.test(rt ~ con, data=_, paired = T) |> 
            tidy() |> 
            mutate(.before=1, n = "rt") |>
            bind_rows(
              test |> 
                t.test(pe ~ con, data=_, paired=T) |> 
                tidy() |> 
                mutate( .before=1, n="pe" ) )
        ) |>
        mutate(.before = 1, 
               pairs   = pairs,
               itr     = itr )
    })
  }, cl=cl) |> 
      map_df(~.x) -> 
    rnd_slice_diag_exclu
  
  # Summarise
  rnd_slice_diag_exclu |>
    summarise(
      .by = c(pairs, name),
      power = mean(p.value<.05), 
      across(where(is.double), mean)
    ) -> sum_rnd_slice_diag_exclu

  # Save
  if(save_simulation_data){
    write_parquet(rnd_slice_diag_exclu, "../data/simulations/exp2_diagnostic_length_sampling_from_exp1_raw.parquet")
    write_parquet(sum_rnd_slice_diag_exclu,"../data/simulations/exp2_diagnostic_length_sampling_from_exp1_sum.parquet")
  }
}
```

```{r vis power, echo=FALSE, warning=FALSE}
sum_rnd_slice_diag_exclu |>
  mutate(
    name = ifelse(name=="pe", "Error rate", "Response time")
  ) |>
  ggplot(aes(pairs, power, col=name)) + 
  geom_line()+
  geom_hline(yintercept=.8)+
  scale_y_continuous(breaks=seq(0,1,.1))+
  geom_vline(xintercept=n_diag_pairs, linetype="dashed")+
  geom_vline(xintercept=n_diag_pairs*loss_trials, linetype="dashed", col="red") + 
  labs(x = "Diagnostic pairs", y = "Power", col="Variable") + 
  theme_bw() +
  theme(legend.position="none")
```

For 27 participants we will have less than 50% power for RT
for the set diagnostic trials `r n_diag_pairs` and even less
for after the exepcted loss `r n_diag_pairs*loss_trials`.

Only PE will be marginally close to 80% power with the
expected available trials `r n_diag_pairs*loss_trials` with
the set post-cue diagnostic length or `r n_diag_pairs`


### Changes in the difference in mean and SD by increasing the number of pairs

How the mean differences for PE and RT change with increase pairs of the diagnostic trial. 
```{r changes in mean difference with increaseing diag, echo=FALSE, warning=FALSE, message=FALSE}
rnd_slice_diag_exclu |> 
  mutate(
    name = ifelse(name=="pe", "Error rate", "Response time")
  ) |>
  ggplot(aes(pairs, m_d, col=name)) +
  stat_summary(geom="line") + 
  geom_vline(xintercept=n_diag_pairs, linetype="dashed") +
  geom_vline(xintercept=n_diag_pairs*loss_trials, linetype="dashed", col="red") +
  labs(x = "Diagnostic pairs", y = "Difference between means", col = "Variable") + 
  scale_y_continuous(breaks=seq(0,2,.15)) +
  coord_cartesian(ylim=c(.10, .75)) +
  theme_bw() + 
  theme(legend.position = "none")
```


```{r SD changes with increase diag, echo=FALSE, message=FALSE, warning=FALSE}
sum_rnd_slice_diag_exclu |> 
  ggplot(aes(pairs, sd_d, col=name))+
  geom_line() +
  geom_vline(xintercept=n_diag_pairs, linetype="dashed")+
  geom_vline(xintercept=n_diag_pairs*loss_trials, linetype="dashed", col="red") + 
  labs(x = "Diagnostic pairs", y = "Standard deviation") +
  scale_y_continuous(breaks=seq(0,2,.25)) +
  theme_bw() +
  theme(legend.position = "none")
```


```{r legends for m-sd , echo=FALSE}
sum_rnd_slice_diag_exclu |>
  mutate(
    name = ifelse(name=="pe", "Error rate", "Response time")
  ) |>
  #pivot_wider(names_from=n, values_from = 3:10) |>
  ggplot(aes(pairs, p.value, col = name)) + 
  labs(col="Variable") +
  theme_bw() + 
  theme(legend.position = "top") +
  geom_line() -> p1
ggpubr::as_ggplot(ggpubr::get_legend(p1))
```



```{r}
#| include: false
sim_pe_rt_parameters <- 
  sum_rnd_slice_diag_exclu |> 
  filter(pairs %in% c(26, 36, 100))
```


## Randomly slicing congruency & and increase sample size

Here I slice only on congruency, and create "hypothetical
persons", by giving them an ID. This means that some "persons"
can contain congruent outcomes from one ID while incongruent
from another ID.

```{r rnd sample and increase}
#| echo: false
if(run_simulations){
  pblapply(27:60, \(samp_s){
    map_df(10:90, \(pairs){
      map_df(1:500, \(itr){
        # We create random "ids" by randomly sampling "samp_s" times,
        # And create a fake id "rnd_id_" 
        map_df(1:samp_s, \(rnd_samp){
          d_ex1 |>
            filter(!is.na(con)) |>
            slice_sample(n = pairs, by = con) |> 
            mutate(id = paste0("rnd_id_", rnd_samp))
        }) |>
          summarise(
            .by = c(id, con),
            rt = mean(rt),
            pe = 1 - mean(correct_response)
          ) -> test
      
        test |>
          t.test(rt ~ con, data=_, paired = T) |> 
          tidy() |> mutate(.before=1, n = "rt") |>
          rbind(
            test |> 
              t.test(pe ~ con, data=_, paired=T) |> 
              tidy() |> mutate(.before=1, n="pe")
          ) |> 
          mutate(
            .before=1, 
            samp  = samp_s,
            pairs = pairs,
            itr   = itr,
          )
      })
    })
  }, cl=cl) |> 
    map_df(~.x) -> 
    rnd_extended_slice_diag_exclu
  
  rnd_extended_slice_diag_exclu |>
    na.omit() |> # omit nas
    summarise(
      .by = c(samp,pairs,n),
      power = mean(p.value<.05), 
      across(where(is.double), mean)
    ) -> sum_rnd_extended_slice_diag_exclu
  
  if(save_simulation_data){
    write_parquet(rnd_extended_slice_diag_exclu, "../data/simulations/exp1_rnd_sampling_inc_samp_size.parquet")
    write_parquet(sum_rnd_extended_slice_diag_exclu, "../data/simulations/exp1_sum_rnd_sampling_inc_samp_size.parquet")
  }
}
```

Power increase over pairs, split by sample size.

```{r}
#| echo: false
sum_rnd_extended_slice_diag_exclu |> 
  mutate( 
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  ggplot(aes(pairs, power, col=samp,group=samp))+
  facet_wrap(~n)+
  geom_line()+
  geom_hline(yintercept=.8)+
  scale_y_continuous(breaks=seq(0,1,.1))+
  scale_x_continuous(breaks=seq(0,100,10))+
  geom_vline(xintercept=n_diag_pairs, linetype="dashed")+
  geom_vline(xintercept=n_diag_pairs*loss_trials, linetype="dashed", col="red") +
  labs(x = "Diagnostic pairs", y = "Power", col = "Sample size", title = "a") +
  theme_bw() + 
  theme(legend.position = "none")
```

Power over sample size split by pairs.

```{r summarise}
#| echo: false
#| warning: false
sum_rnd_extended_slice_diag_exclu |>
  mutate( 
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  ggplot(aes(samp, power, col=pairs,group=pairs))+
  facet_wrap(~n)+
  geom_hline(yintercept=.8)+
  geom_line() +
  scale_y_continuous(breaks=seq(0,1,.2))+
  scale_x_continuous(breaks=seq(0,100,10))+
  labs(x = "Sample size", y = "Power", col = "Diagnostic pairs", title="b") +
  theme_bw() + 
  theme(legend.position = "none")
```

Power over sample size split by 36 and 26 pairs of
observations.

```{r legend for other stuff, echo=FALSE}
sum_rnd_extended_slice_diag_exclu |> 
  mutate( 
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  ggplot(aes(pairs, power, col=samp,group=samp))+
  facet_wrap(~n)+
  geom_line()+
  labs(x = "Diagnostic pairs", y = "Power", col = "Sample size") +
  theme_bw() + 
  theme(legend.position = "top") -> p2
ggpubr::as_ggplot(ggpubr::get_legend(p2))
```


#### Split by relevance


```{r}
#| echo: false
sum_rnd_extended_slice_diag_exclu |> 
  mutate( 
    n = ifelse(n=="pe", "Error rate", "Response time")
  ) |>
  filter(pairs %in% c(n_diag_pairs, round(n_diag_pairs*loss_trials))) |>
  ggplot(aes(samp, power, col=factor(pairs),group=factor(pairs)))+
  facet_wrap(~n)+
  geom_line()+ 
  geom_hline(yintercept=.8)+
  geom_hline(yintercept=.9, linetype="dashed", col="darkblue")+
  geom_vline(xintercept=34) +
  labs(x = "Sample size", y = "Power", col = "Diagnostic pairs") +
  theme_bw() + 
  theme(legend.position = "top")
```

Random slicing from the first experiment suggests that we
would need slightly more than 30 participants (34) to achieve
80% power for PE. RT, on the other hand, remains below 50% for
after the expected loss. In other words, increasing the sample
size will not remedy the problem for RT.

One way to remedy this problem is by investigating the utility
of the integrated scores.

## Randomly slicing with integrated score

```{r}
#| echo: false
if(run_simulations){
  clusterExport(cl, c("d_ex1"))
  pblapply(22:82, \(samp_s){
    map_df(10:120, \(pairs){
      map_df(1:500, \(itr){
        # We create random "ids" by randomly sampling "samp_s" times,
        # And create a fake id "rnd_id_" 
        map_df(1:samp_s, \(rnd_samp){
          d_ex1 |>
            filter(inducer_run>0) |>
            filter(!is.na(con)) |>
            slice_sample(n = pairs, by = con) |> 
            rename(original_id = id) |>
            mutate(id = paste0("rnd_id_", rnd_samp))
        }) -> test

        # Grand:
      grand  <-  test |>
        summarize(
          g_rt = mean(rt),
          g_rt_sd = sd(rt),
          g_pc = mean(correct_response),
          g_pe = 1 - g_pc,
          g_pc_sd =  sd(correct_response) )
      
      # LISAS:
      lisas  <-  test |>
        summarize(
          .by = id,
          l_rt_sd = sd(rt),
          l_pe_s  = sqrt( mean(correct_response) * (1 - mean(correct_response)) )
        )
        
      # Calculation
      res  <-  test |>
        summarise(
          .by = c(id, con), 
          rt_sd = sd(rt),
          rt = mean(rt),
          pe_sd = sd(correct_response),
          pe = 1 - mean(correct_response),
          pc = mean(correct_response),
        ) |> 
        left_join(lisas, by="id") |> 
        cbind(grand) |>
        as_tibble() |>
        mutate(
          lisas  = ifelse(is.infinite(l_rt_sd/l_pe_s), rt, rt + (l_rt_sd/l_pe_s) * pe), 
          BIS    = ( (pc - g_pc) / g_pc_sd ) - ( (rt - g_rt) / g_rt_sd ) )
      
      res |>
        t.test(lisas ~ con, data=_, paired = T) |> 
        tidy() |> 
        mutate(.before=1, n = "LISAS") |>
        rbind(
          res |> 
            t.test(BIS ~ con, data=_, paired=T) |> 
            tidy() |> 
            mutate(.before=1, n="BIS")
        ) |> 
        mutate(
          .before=1, 
          samp  = samp_s,
          pairs = pairs,
          itr   = itr,
        ) |>
        left_join(
          by = join_by("n"=="name"),
          res |> 
            select(id,con,lisas,BIS) |> 
            mutate(lisas = scale(lisas)) |>
            rename(LISAS = lisas) |>
            pivot_longer(c(LISAS, BIS)) |>
            pivot_wider(names_from=con, values_from=value) |>
            summarise(
              .by = name,
              m_diff    = mean(`FALSE`-`TRUE`),
              sd_diff   = sd(`FALSE`-`TRUE`),
            )
        ) 
    })
  })
}, cl = cl) |> 
  map_df(~.x) -> 
  rnd_int_scores_extended_slice_diag_exclu

rnd_int_scores_extended_slice_diag_exclu |>
    summarise(
      .by = c(samp,pairs,n),
      power = mean(p.value<.05), 
      across(where(is.double), mean)
    ) -> sum_rnd_int_scores_extended_slice_diag_exclu
  
  if(save_simulation_data){
    write_parquet(rnd_int_scores_extended_slice_diag_exclu, "../data/simulations/exp1_rnd_sampling_inc_samp_size_integrated_scores.parquet")
    write_parquet(sum_rnd_int_scores_extended_slice_diag_exclu, "../data/simulations/exp1_sum_rnd_sampling_inc_samp_size_integrated_scores.parquet")
  }
}
```

```{r summarise integrated score}
#| echo: false
#| warning: false
#| fig-dpi: 300
#| fig-width: 7
sum_rnd_int_scores_extended_slice_diag_exclu |>
  ggplot(aes(samp, power, col=pairs,group=pairs))+
  facet_wrap(~n)+
  geom_hline(yintercept=.8)+
  geom_line() +
  labs(x = "Sample size", y = "Power", col = "Diagnostic pairs") +
  theme_bw() 
```

```{r integrated scores summary of relevant pairs}
#| echo: false
#| fig-dpi: 300
#| fig-width: 6
sum_rnd_int_scores_extended_slice_diag_exclu |> 
  filter(pairs %in% c(n_diag_pairs, round(n_diag_pairs*loss_trials))) |>
  ggplot(aes(samp, power, col=factor(pairs),group=factor(pairs)))+
  facet_wrap(~n) +
  geom_line() + 
  geom_hline(yintercept=.8) +
  geom_hline(yintercept=.9, linetype="dashed", col="darkblue") +
  geom_vline(xintercept=34) +
  labs(x = "Sample size", y = "Power", col = "Diagnostic pairs") +
  theme_bw() 
```

Relying on the BIS or LISAS will give us at least 80% power
with 30 participants, but around 90% power with 34
participants.

--------------------------------------------------------------

~Document\ generated:\ `r Sys.time()`~
