---
title: "exp2_diag_sim_new"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---


# Sampling from experiment 1

First we can sample effects from experiment 1.

```{r libraries}
#| include: false

library(tidyverse)
library(lsr)
library(pbapply)
library(afex)
library(gt)
library(gganimate)
library(gifski)
library(av)
library(arrow) # load parquet files 
```

```{r params}
#| include: false

save_animations <- FALSE #?

run_simulations <- FALSE
save_simulation_data <- FALSE # if false, will load stored data
```

```{r load data}
#| include: false
load("../data/processed/exp1_data.rdata")
load("../data/simulations/diag_simu_sum.rdata")

if(!save_simulation_data){
  finding_exp1_parameter_vals <- read_parquet("../data/simulations/finding_parameter_values_for_exp1.parquet")
  vary_diagnostic_lengths <- read_parquet("../data/simulations/vary_diagnostic_length.parquet")
}

```

# Experiment 1: 

```{r data}
#| echo: false
d <-
  exp1_d |>
  dplyr::select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, con) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") |>
  filter(!is.na(correct_response)) |>
  mutate(rt = as.integer(rt)) 
```

```{r excluding data}
#| echo: false
d -> d_ex
loss <- list()
loss$data_trials <- nrow(d_ex)

# accur
d_ex |>
  filter( trial_info == "Diagnostic trial" | trial_info == "Inducer trial" ) |>
  group_by( id ) |>
  mutate( correct_response = ifelse( trial_info=="Inducer trial" & is.na(rt), 0, correct_response) ) |>
    #' !  Non-responses count as a wrong response & are subject to the exclusion criteria.  !
  summarise( acc = sum(correct_response, na.rm = TRUE) / length( !is.na(correct_response) ) ) |>
  filter( acc < .7 ) |>
  pull( id ) -> loss$exclude_par

loss[["exclude_par_trials"]] <- length( d_ex$rt[d_ex$id == loss$exclude_par] )
loss[["exclude_par_pct"]]    <- length( d_ex$rt[d_ex$id == loss$exclude_par] ) / loss$data_trials * 100

d_ex |> filter( !(id %in% loss$exclude_par) ) -> d_ex
# rt dev+miss
d_ex |>
  mutate(rt = as.integer(rt)) |>
  ungroup() |>
  group_by(id) |>
  mutate(
    rt_crit = ifelse( 
      trial_info == "Diagnostic trial",
      mean( rt ) + sd( rt ) * 2.5,
      NA ),
      retain_trials = ifelse(
        (rt > rt_crit & trial_info == "Diagnostic trial") | # **OR**
          (is.na(rt) & trial_info == "Diagnostic trial"),
        0, 1 ) 
    ) -> d_ex

sum(d_ex$retain_trials == 0) -> loss[["rt_sd_trials"]]
sum(d_ex$retain_trials == 0) / loss$data_trials * 100 -> loss[["rt_sd_pct"]]

d_ex |> 
  filter( retain_trials == 1 ) -> d_ex
# inducers:
d_ex |>
  mutate( valid_trials = case_when( trial_info=="Inducer trial" & correct_response==1 ~ 1,
                                    trial_info=="Inducer trial" & correct_response==0 ~ 0,
                                    trial_info=="Inducer trial" & is.na(correct_response) ~ 0,
                                      # non-responses count as a wrong response
                                    T ~ NA ) ) |>
  fill(valid_trials, .direction = "up") -> d_ex

sum(d_ex$valid_trials==0) -> loss[["inducer_fail_trials"]]
sum(d_ex$valid_trials==0) / loss$data_trials * 100 -> loss[["inducer_fail_pct"]]

d_ex |> 
  filter( valid_trials == 1 ) |> 
  ungroup() -> d_ex
```

## Estimating lost trials: 
```{r}
#| include: false
d_ex |>
  filter(trial_info=="Diagnostic trial") |>
  group_by(id) |>
  count() |>
  ungroup() |>
  pull(n) |> 
  mean() -> exp1_remaining_trials
```

After the exclusion criteria, we are left with `r exp1_remaining_trials` trials.

This corresponds to a loss of `r exp1_remaining_trials/240` (or `r exp1_remaining_trials/240*100` percent). We will assume a loss of .75 (75%). 

In other words, whatever post-cue run we decide, we need to estimate an expected loss of about .75. 


# Parameter from exp1:

## exp1 outcomes values 

Calculate the standardized means and standard deviations, and effect sizes from experiment 1. 

```{r}
#| echo: false

d_ex |> 
  ungroup() |>
  filter(!is.na(con)) |>
  summarise(
    rt = mean(rt), 
    pe = 1 - mean(correct_response),
    .by = c(id, con)
  ) |> 
  mutate(
    rt = scale(rt),
    pe = scale(pe)
  ) |>
  pivot_wider(names_from = con, values_from = c(rt,pe)) |>
  summarise(
    rt_d_m = mean(rt_FALSE-rt_TRUE),
    rt_d_sd = sd(rt_FALSE-rt_TRUE),
    rt_d = rt_d_m / rt_d_sd,
    pe_d_m = mean(pe_FALSE-pe_TRUE),
    pe_d_sd = sd(pe_FALSE-pe_TRUE),
    pe_d = pe_d_m / pe_d_sd,
  ) -> exp1_outcome
exp1_outcome
```

## equivalent ANOVA value (GES) 

```{r}
#| echo: false
d_ex |> 
  filter(!is.na(con)) |>
  summarise(
    .by = c(id, con),
    rt = mean(rt),
    pe = 1 - mean(correct_response)
  ) -> d_ex_sum
d_ex_sum |>
  aov_car(rt ~ con + Error(id/con), data = _) |>
  pluck("anova_table") |>
  as.tibble() |>
  mutate( name = "rt", .before = 1) |>
  rbind( 
    d_ex_sum |>
      aov_car(pe ~ con + Error(id/con), data = _) |> 
      pluck("anova_table") |>
      as.tibble() |>
      mutate( name = "pe",.before = 1)
    ) -> exp1_anova_outcome
exp1_anova_outcome
```

RT has a GES of 0.00848
PE has a GES of 0.117 

## Simulate exp1 data

```{r}
#| echo: false
gen_data <- function(N, pair.obs, eff.rt, sd.rt, eff.pe, sd.pe){
  #' @param N Number of subjects
  #' @param OBS Number of observation per N
  #' @param eff.rt  Effect size of congruency on response time
  #' @param eff.pct Effect size of congruency on proportion of correct trials 

  tibble(
    id  = rep(1:N, each = pair.obs*2),
    con = rep.int( c("con","incon"), N * pair.obs),
    rt  = rnorm(N * pair.obs * 2, 
                0 + as.integer(con=="con") * eff.rt, 
                sd.rt),
    pe  = rnorm(N * pair.obs * 2, 
                0 + as.integer(con=="con") * eff.pe, 
                sd.pe),
  ) 
}
```

Simulate data with exp1 outcomes:

```{r quick sim}
#| echo: false

map_df(1:20, \(x){
  gen_data(
    27,
    x,
    exp1_outcome$rt_d_m,
    exp1_outcome$rt_d_sd,
    exp1_outcome$pe_d_m,
    exp1_outcome$pe_d_sd
  ) -> single_dat_sim

  single_dat_sim |> 
    summarise(
      .by = c(id, con),
      rt_m = mean(rt),
      pe_m = mean(pe)
    ) -> single_dat_calc
  
  sim_aov_rt <- 
    aov_car(rt_m~con + Error(id/(con)), data=single_dat_calc)
  sim_t_rt <- 
    t.test(rt_m~con, data=single_dat_calc , paired=T, alterantive="greater") 
  
  single_dat_calc |>
    pivot_wider(names_from=con, values_from = c(rt_m, pe_m)) |>
    summarise(
      num = x, 
      m = mean(rt_m_con - rt_m_incon),
      sd = sd(rt_m_con - rt_m_incon),
      d = m / sd,
      t_rt_p = sim_t_rt$p.value,
      aov_rt_p = sim_aov_rt$anova_table$`Pr(>F)`,
      aov_rt_ges = sim_aov_rt$anova_table$ges,
    )
})
```

Increasing the amount of observations merely decreases the SD, making all results more significant. 
The only value that is approximating our results is the simulation with 1 (pair of) observation(s). 

For this reason, we need to simulate the amount of observations we have (~ 180 valid trials) and estimate the experiment parameters that create the results found in experiment 1. 


```{r find exp1 parameters}
#| echo: false
N = 27
OBS = 180/2
eff.rt = .17
eff.pe = .68

library(parallel)
library(arrow)
library(broom)
#library(tidymodels)

if(run_simulations){
  parallel::makeCluster( detectCores()*.9 ) -> cl
  clusterExport(cl, varlist = c("N", "OBS", "eff.rt", "eff.pe"))
   # Initiate libraries in clusters
  clusterEvalQ(cl, {
    library(tidyverse)
    library(pbapply)
    library(lsr)
    library(broom)
  })
  
  pblapply(seq(1,10,.02), \(x){
    
    map_df(1:200, \(itr){
      
      tibble(
        id  = rep(1:N, each = OBS*2),
        con = rep.int( c("con","incon"), N*OBS),
        rt  = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.rt, x),
        pe  = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.pe, x),
      ) |> 
        summarise( 
          .by = c(id, con),
          rt = mean(rt), 
          pe = mean(pe)
        ) -> trans
        
      
      trans |>
        pivot_wider(names_from=con, values_from=c(rt,pe)) |>
        summarise( 
          rt = mean(rt_con-rt_incon),
          rt_sd = sd(rt_con-rt_incon),
          rt_d = rt / rt_sd,
          pe = mean(pe_con-pe_incon),
          pe_sd = sd(pe_con-pe_incon),
          pe_d = pe/pe_sd,
        ) -> val
      
      trans |>
        t.test(rt ~ con, data = _, paired = T) |>
        broom::tidy() -> rt_test
      
      names(rt_test) <- paste0("rt_", names(rt_test)) 
      
      trans |>
        t.test(pe~con, data=_, paired=T) |>
        broom::tidy() -> pe_test
      
      rt_test |>
        cbind(pe_test) |>
        cbind(val) |>
        mutate(itr = itr,
               sd = x) 
    })
  }, cl = cl) |> 
    map_df(~.x) -> finding_exp1_parameter_vals
  
  write_parquet(finding_exp1_parameter_vals, sink = "../data/simulations/finding_parameter_values_for_exp1.parquet")
  
  stopCluster(cl)
}
```

```{r}
#| echo: false

finding_exp1_parameter_vals |>
  select(rt:sd, contains("p.value")) |>
  summarise(
    .by = sd, 
    pe_pow = mean(p.value < .05),
    rt_pow = mean(rt_p.value < .05),
    across(everything(), mean)
  )
```

Closest RT parameters: mdiff = .166, sd = 2     (d = .58, pow = 80%).
Closest PE parameters: mdiff = .65,  sd = 7.6   (d = .59, pow = 80%).

That is, these parameter values correspond to the results from experiment 1 *with* 180 trials (or 90 pairs; assuming we have exactly 90 pairs). 

With this, we can simulate data for the experiment *with less trials*. 

Only the these parameter may highlight the problem with lacking trials (i.e., post-cue trial) for the post-cue diagnostic run. 


Simulate data with the paramaters as estimated above: 

```{r}
#| echo: false

l <- 
list( 
  rt_m  = exp1_outcome$rt_d_m,
  rt_sd = 2,
  pe_m  = exp1_outcome$pe_d_m,
  pe_sd = 7.6
)

if(run_simulations){
  
  makeCluster(detectCores() * .9) -> cl
  clusterExport(cl, c("l", "gen_data"))
  clusterEvalQ(cl, {
    library(tidyverse)
    library(broom)
  })
  
  pblapply(seq(1, 90, 1), \(pairs){
    
    map_df(1:300, \(itr){
      gen_data(27, pairs, l$rt_m, l$rt_sd, l$pe_m, l$pe_sd) |> 
        summarise(
          .by = c(id, con), 
          rt = mean(rt),
          pe = mean(pe)
        ) -> s_d
      
      s_d |>
        pivot_wider(names_from = con, values_from = c(rt,pe)) |>
        summarise(
          rt_m = mean(rt_con-rt_incon),
          rt_sd = sd(rt_con-rt_incon),
          rt_d = rt_m / rt_sd,
          pe_m = mean(pe_con-pe_incon),
          pe_sd = sd(pe_con-pe_incon),
          pe_d = pe_m / pe_sd,
        ) |> 
        cbind(
          t.test(rt ~ con, s_d, paired=T, alternative="greater") |>
            tidy() |> 
            select(2:3) |>
            rename(rt_stat = statistic,
                   rt_p = p.value)
        ) |>
        cbind( 
          t.test(pe ~ con, s_d, paired=T, alternative="greater") |>
            tidy() |> 
            select(2:3) |>
            rename(pe_stat = statistic,
                   pe_p = p.value)
        ) |>
        mutate(
          pairs = pairs,
          itr = itr, 
        ) |>
          select(pairs, itr, starts_with("rt"), starts_with("pe")
        )
    })
    
  }, cl = cl) |> 
    map_df(~.x) -> vary_diagnostic_lengths
  
  if(save_simulation_data){
    write_parquet(vary_diagnostic_lengths, "../data/simulations/vary_diagnostic_length.parquet")
  }
  
  stopCluster(cl)
} 
```


```{r}
#| echo: false
vary_diagnostic_lengths |> 
  summarise(
    .by = pairs,
    rt_pow = mean(rt_p<.05),
    pe_pow = mean(pe_p<.05), 
    across(c(starts_with("rt_"), starts_with("pe_")), mean)
  ) -> sum_vary_diag_len
sum_vary_diag_len
```


The relative power with respect to the simulated data as calculated from the estimate parameters. 

```{r}
#| echo: false

sum_vary_diag_len |> 
  pivot_longer(c(rt_pow, pe_pow)) |>
  ggplot(aes(pairs, value, col=name))+
  geom_line()+
  scale_x_continuous(breaks=seq(0,90,10))+
  scale_y_continuous(breaks=seq(0,1,.1))+
  geom_hline(yintercept = .8)+
  geom_vline(xintercept = 35, linetype="dashed")+
  geom_vline(xintercept = 35 * .75, linetype="dashed", col="red")
```

According to the simulation, it would suffice with 60 pairs (120 trials) to have 80% power to detect a true effect.

However, assuming we have 70 trials in the post-cue run, we will have 35 (forced) pairs. As we will lose 25% of the trials, we are left with 26.25 pairs, which has a power close to 50%. 

We might remedy this problem by increasing the sample size. 



```{r}
#| echo: false

l <- 
  list( 
    rt_m  = exp1_outcome$rt_d_m,
    rt_sd = 2,
    pe_m  = exp1_outcome$pe_d_m,
    pe_sd = 7.6
  )

if(run_simulations){
  
  makeCluster(detectCores() * .9) -> cl
  clusterExport(cl, c("l", "gen_data"))
  clusterEvalQ(cl, {
    library(tidyverse)
    library(broom)
  })
  
  pblapply(20:100, \(size){
    
    map_df(seq(1, 90, 1), \(pairs){
      
      map_df(1:300, \(itr){
        gen_data(size, pairs, l$rt_m, l$rt_sd, l$pe_m, l$pe_sd) |> 
          summarise(
            .by = c(id, con), 
            rt = mean(rt),
            pe = mean(pe)
          ) -> s_d
        
        s_d |>
          pivot_wider(names_from = con, values_from = c(rt,pe)) |>
          summarise(
            rt_m = mean(rt_con-rt_incon),
            rt_sd = sd(rt_con-rt_incon),
            rt_d = rt_m / rt_sd,
            pe_m = mean(pe_con-pe_incon),
            pe_sd = sd(pe_con-pe_incon),
            pe_d = pe_m / pe_sd,
          ) |> 
          cbind(
            t.test(rt ~ con, s_d, paired=T, alternative="greater") |>
              tidy() |> 
              select(2:3) |>
              rename(rt_stat = statistic,
                     rt_p = p.value)
          ) |>
          cbind( 
            t.test(pe ~ con, s_d, paired=T, alternative="greater") |>
              tidy() |> 
              select(2:3) |>
              rename(pe_stat = statistic,
                     pe_p = p.value)
          ) |>
          mutate(
            pairs = pairs,
            itr = itr, 
          ) |>
            select(pairs, itr, starts_with("rt"), starts_with("pe")
          )
      })
    })
  }, cl = cl) |> 
    map_df(~.x) -> diag_lens_over_samp_sizes
  
  if(save_simulation_data){
    write_parquet(vary_diagnostic_lengths, "../data/simulations/vary_diagnostic_length.parquet")
  }
  
  stopCluster(cl)
} 
```




```{r}
gen_data(
  27,
  5,
  exp1_outcome$rt_d_m,
  exp1_outcome$rt_d_sd,
  exp1_outcome$pe_d_m,
  exp1_outcome$pe_d_sd
) -> single_dat_sim

single_dat_sim |> 
  summarise(
    .by = c(id, con),
    rt_m = mean(rt),
    pe_m = mean(pe)
  ) -> single_dat_calc
aov_car(rt_m~con + Error(id/(con)), data=single_dat_calc) # p = .035, GES = .09
t.test(rt_m~con, data=single_dat_calc, paired=T) #          p = .035,  d = .42
single_dat_calc |>
  pivot_wider(names_from=con, values_from = c(rt_m, pe_m)) |>
  summarise(
      m = mean(rt_m_con - rt_m_incon),
      sd = sd(rt_m_con - rt_m_incon),
      d = m / sd
    )
```



```{r}
#| echo: false

single_dat_sim |> 
  summarise(
    .by = c(id, con),
    rt_m = mean(rt),
    pe_m = mean(pe)
  ) |>
  pivot_wider(names_from = con, values_from = c(rt_m, pe_m))  |>
  summarise(
    rt_md = mean(rt_m_incon - rt_m_con),
    rt_sd = sd(rt_m_incon - rt_m_con),
    pe_md = mean(pe_m_incon - pe_m_con),
    pe_sd = sd(pe_m_incon - pe_m_con),
  )

  pivot_wider(names_from = con, values_from = c(rt_m, pe_m)) |>
  summarise(
    rt_d = (rt_m_con - rt_m_incon),
    m_rt = mean(rt_d),
    sd_rt = sd(rt_d),
    d = m_rt/sd_rt
  )

d_ex |> 
  filter(diagnostic_run > 0) |>
  mutate(rt = scale(rt)) |>
  summarise(
    .by=c(id,con,inducer_run),
    rt = mean(rt),
    rt_sd = sd(rt),
    pe = mean(correct_response)
  ) |>
  mutate( pe = scale(pe) )
  pivot_wider(names_from = con, values_from=c(rt, correct_response))

```


only output is displayed).
