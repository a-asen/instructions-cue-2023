---
title: "exp2_diag_sim_new"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---


# Sampling from experiment 1

First we can sample effects from experiment 1.

```{r}
#| include: false

library(tidyverse)
library(lsr)
library(pbapply)
library(afex)
library(gt)
library(gganimate)
library(gifski)
library(av)
library(arrow) # load parquet files 

save_animations <- FALSE
run_simulations <- FALSE
load_big_data <- FALSE
```

```{r load data}
#| include: false
load("../data/processed/exp1_data.rdata")
load("../data/simulations/diag_simu_sum.rdata")

if(load_big_data){ load("../data/simulations/diag_simu_raw.rdata") }
read_parquet("../data/simulations/finding_parameter_values_for_exp1.parquet") -> test
```

```{r}
#| echo: false
d <-
  exp1_d |>
  dplyr::select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, con) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") |>
  filter(!is.na(correct_response)) |>
  mutate(rt = as.integer(rt)) 
```

```{r loss est}
#| echo: false
d -> d_ex
loss <- list()
loss$data_trials <- nrow(d_ex)

# accur
d_ex |>
  filter( trial_info == "Diagnostic trial" | trial_info == "Inducer trial" ) |>
  group_by( id ) |>
  mutate( correct_response = ifelse( trial_info=="Inducer trial" & is.na(rt), 0, correct_response) ) |>
    #' !  Non-responses count as a wrong response & are subject to the exclusion criteria.  !
  summarise( acc = sum(correct_response, na.rm = TRUE) / length( !is.na(correct_response) ) ) |>
  filter( acc < .7 ) |>
  pull( id ) -> loss$exclude_par

loss[["exclude_par_trials"]] <- length( d_ex$rt[d_ex$id == loss$exclude_par] )
loss[["exclude_par_pct"]]    <- length( d_ex$rt[d_ex$id == loss$exclude_par] ) / loss$data_trials * 100

d_ex |> filter( !(id %in% loss$exclude_par) ) -> d_ex
# rt dev+miss
d_ex |>
  mutate(rt = as.integer(rt)) |>
  ungroup() |>
  group_by(id) |>
  mutate(
    rt_crit = ifelse( 
      trial_info == "Diagnostic trial",
      mean( rt ) + sd( rt ) * 2.5,
      NA ),
      retain_trials = ifelse(
        (rt > rt_crit & trial_info == "Diagnostic trial") | # **OR**
          (is.na(rt) & trial_info == "Diagnostic trial"),
        0, 1 ) 
    ) -> d_ex

sum(d_ex$retain_trials == 0) -> loss[["rt_sd_trials"]]
sum(d_ex$retain_trials == 0) / loss$data_trials * 100 -> loss[["rt_sd_pct"]]

d_ex |> 
  filter( retain_trials == 1 ) -> d_ex
# inducers:
d_ex |>
  mutate( valid_trials = case_when( trial_info=="Inducer trial" & correct_response==1 ~ 1,
                                    trial_info=="Inducer trial" & correct_response==0 ~ 0,
                                    trial_info=="Inducer trial" & is.na(correct_response) ~ 0,
                                      # non-responses count as a wrong response
                                    T ~ NA ) ) |>
  fill(valid_trials, .direction = "up") -> d_ex

sum(d_ex$valid_trials==0) -> loss[["inducer_fail_trials"]]
sum(d_ex$valid_trials==0) / loss$data_trials * 100 -> loss[["inducer_fail_pct"]]

d_ex |> 
  filter( valid_trials == 1 ) |> 
  ungroup() -> d_ex
```

The above table indicate the number of diagnostic trials that are left after all excluding trials. 

On average, this leaves us with:
```{r}
#| echo: false
d_ex |>
  filter(trial_info=="Diagnostic trial") |>
  group_by(id) |>
  count() |>
  ungroup() |>
  summarise(avg = mean(n))
```
trials.

This is a loss of approximately: `r 180/240`. Whatever post-cue run we decide, we need to estimate an expected loss of about .75. 


# Parameter from exp1:

Calculate the standardized means and standard deviations, and effect sizes from experiment 1. 

```{r}
#| echo: false

d_ex |> 
  ungroup() |>
  filter(!is.na(con)) |>
  summarise(
    rt = mean(rt), 
    pe = 1 - mean(correct_response),
    .by = c(id, con)
  ) |> 
  mutate(
    rt = scale(rt),
    pe = scale(pe)
  ) |>
  pivot_wider(names_from = con, values_from = c(rt,pe)) |>
  summarise(
    rt_d_m = mean(rt_FALSE-rt_TRUE),
    rt_d_sd = sd(rt_FALSE-rt_TRUE),
    rt_d = rt_d_m / rt_d_sd,
    pe_d_m = mean(pe_FALSE-pe_TRUE),
    pe_d_sd = sd(pe_FALSE-pe_TRUE),
    pe_d = pe_d_m / pe_d_sd,
  ) -> exp1_outcome
exp1_outcome
```

```{r}
d_ex |> 
  summarise(
    .by = c(id, con),
    rt = mean(rt),
    pe = 1 - mean(correct_response)
  ) |> 
  t.test(rt ~ con, data = _, paired = T)
```


These values are standardized, meaning these are the values we will simulate with (?).

```{r}
#| echo: false
gen_data <- function(N, pair.obs, eff.rt, sd.rt, eff.pe, sd.pe){
  #' @param N Number of subjects
  #' @param OBS Number of observation per N
  #' @param eff.rt  Effect size of congruency on response time
  #' @param eff.pct Effect size of congruency on proportion of correct trials 

  tibble(
    id  = rep(1:N, each = pair.obs * 2),
    con = rep.int( c("con","incon"), N * pair.obs),
    rt  = rnorm(N * pair.obs * 2, 
                0 + as.integer(con=="con") * eff.rt, 
                sd.rt),
    pe  = rnorm(N * pair.obs * 2, 
                0 + as.integer(con=="con") * eff.pe, 
                sd.pe),
  ) 
}
```

Effect size of about .41 equals GES of .092. 

```{r}
#| echo: false
N = 27
OBS = 180/2
eff.rt = .17
eff.pe = .68

library(parallel)
library(arrow)
library(broom)
#library(tidymodels)

if(run_simulations){
  parallel::makeCluster( detectCores()*.9 ) -> cl
  clusterExport(cl, varlist = c("N", "OBS", "eff.rt", "eff.pe"))
   # Initiate libraries in clusters
  clusterEvalQ(cl, {
    library(tidyverse)
    library(pbapply)
    library(lsr)
    library(broom)
  })
  
  pblapply(seq(1,10,.02), \(x){
    
    map_df(1:200, \(itr){
      
      tibble(
        id  = rep(1:N, each = OBS*2),
        con = rep.int( c("con","incon"), N*OBS),
        rt  = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.rt, x),
        pe  = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.pe, x),
      ) |> 
        summarise( 
          .by = c(id, con),
          rt = mean(rt), 
          pe = mean(pe)
        ) -> trans
        
      
      trans |>
        pivot_wider(names_from=con, values_from=c(rt,pe)) |>
        summarise( 
          rt = mean(rt_con-rt_incon),
          rt_sd = sd(rt_con-rt_incon),
          rt_d = rt / rt_sd,
          pe = mean(pe_con-pe_incon),
          pe_sd = sd(pe_con-pe_incon),
          pe_d = pe/pe_sd,
        ) -> val
      
      trans |>
        t.test(rt ~ con, data = _, paired = T) |>
        broom::tidy() -> rt_test
      
      names(rt_test) <- paste0("rt_", names(rt_test)) 
      
      trans |>
        t.test(pe~con, data=_, paired=T) |>
        broom::tidy() -> pe_test
      
      rt_test |>
        cbind(pe_test) |>
        cbind(val) |>
        mutate(itr = itr,
               sd = x) 
    })
  }, cl = cl) |> 
    map_df(~.x) -> finding_exp1_parameter_vals
  
  write_parquet(finding_exp1_parameter_vals, sink = "../data/simulations/finding_parameter_values_for_exp1.parquet")
  
  stopCluster(cl)
}
```

```{r}
#| echo: false

finding_exp1_parameter_vals |>
  select(rt:sd, contains("p.value")) |>
  summarise(
    .by = sd, 
    pe_pow = mean(p.value < .05),
    rt_pow = mean(rt_p.value < .05),
    across(everything(), mean)
  )
```

Closest RT parameters: mdiff = .166, sd = 2     (d = .58, pow = 80%).
Closest PE parameters: mdiff = .65,  sd = 7.6   (d = .59, pow = 80%).

That is, these parameter values correspond to the results from experiment 1 *with* 180 trials (or 90 pairs; assuming we have exactly 90 pairs). 

With this, we can simulate data for the experiment *with less trials*. 

Only the these parameter may highlight the problem with lacking trials for the post-cue diagnostic run. 


```{r}
#| echo: false

rt_m <- exp1_outcome$rt_d_m
rt_sd <- 2
pe_m <- exp1_outcome$pe_d_m
pe_sd <- 7.6 


```




```{r}
gen_data(
  27,
  5,
  exp1_outcome$rt_d_m,
  exp1_outcome$rt_d_sd,
  exp1_outcome$pe_d_m,
  exp1_outcome$pe_d_sd
) -> single_dat_sim

single_dat_sim |> 
  summarise(
    .by = c(id, con),
    rt_m = mean(rt),
    pe_m = mean(pe)
  ) -> single_dat_calc
aov_car(rt_m~con + Error(id/(con)), data=single_dat_calc) # p = .035, GES = .09
t.test(rt_m~con, data=single_dat_calc, paired=T) #          p = .035,  d = .42
single_dat_calc |>
  pivot_wider(names_from=con, values_from = c(rt_m, pe_m)) |>
  summarise(
      m = mean(rt_m_con - rt_m_incon),
      sd = sd(rt_m_con - rt_m_incon),
      d = m / sd
    )
```



```{r}
#| echo: false

single_dat_sim |> 
  summarise(
    .by = c(id, con),
    rt_m = mean(rt),
    pe_m = mean(pe)
  ) |>
  pivot_wider(names_from = con, values_from = c(rt_m, pe_m))  |>
  summarise(
    rt_md = mean(rt_m_incon - rt_m_con),
    rt_sd = sd(rt_m_incon - rt_m_con),
    pe_md = mean(pe_m_incon - pe_m_con),
    pe_sd = sd(pe_m_incon - pe_m_con),
  )

  pivot_wider(names_from = con, values_from = c(rt_m, pe_m)) |>
  summarise(
    rt_d = (rt_m_con - rt_m_incon),
    m_rt = mean(rt_d),
    sd_rt = sd(rt_d),
    d = m_rt/sd_rt
  )

d_ex |> 
  filter(diagnostic_run > 0) |>
  mutate(rt = scale(rt)) |>
  summarise(
    .by=c(id,con,inducer_run),
    rt = mean(rt),
    rt_sd = sd(rt),
    pe = mean(correct_response)
  ) |>
  mutate( pe = scale(pe) )
  pivot_wider(names_from = con, values_from=c(rt, correct_response))

```


only output is displayed).
