---
title: "exp2_diag_sim_new"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---

```{r libraries}
#| include: false

library(tidyverse)
library(lsr)
library(pbapply)
library(afex)
library(gt)
library(gganimate)
library(gifski)
library(av)
library(arrow) # load parquet files 
```

```{r params}
#| include: false

save_animations <- FALSE #?
run_simulations <- FALSE

save_simulation_data <- FALSE
load_simulation_data <- FALSE
```

```{r load data}
#| include: false
load("../data/processed/exp1_data.rdata")
load("../data/simulations/diag_simu_sum.rdata")

if(load_simulation_data){
  finding_exp1_parameter_vals <- read_parquet("../data/simulations/finding_parameter_values_for_exp1.parquet")
  vary_diagnostic_lengths <- read_parquet("../data/simulations/vary_diagnostic_length.parquet")
  rnd_slice_diag_exclu <- read_parquet("../data/simulations/exp2_diagnostic_length_sampling_from_exp1_raw.parquet")
  sum_rnd_slice_diag_exclu <- read_parquet("../data/simulations/exp2_diagnostic_length_sampling_from_exp1_sum.parquet")
  rnd_extended_slice_diag_exclu <- read_parquet("../data/simulations/exp1_rnd_sampling_inc_samp_size.parquet")
  sum_rnd_extended_slice_diag_exclu <- read_parquet("../data/simulations/exp1_sum_rnd_sampling_inc_samp_size.parquet")
  
  #    write_parquet(diag_lens_over_samp_sizes, "../data/simulations/diag_lens_over_sample_sizes.parquet")
    #write_parquet(sum_diag_lens_over_samp_sizes, "../data/simulations/diag_lens_over_sample_sizes_sum.parquet")
}

```



# Experiment 1: 

```{r data}
#| echo: false
d <-
  exp1_d |>
  dplyr::select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, con) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") |>
  filter(!is.na(correct_response)) |>
  mutate(rt = as.integer(rt)) 
```

```{r excluding data}
#| echo: false
d -> d_ex
loss <- list()
loss$data_trials <- nrow(d_ex)

# accur
d_ex |>
  filter( trial_info == "Diagnostic trial" | trial_info == "Inducer trial" ) |>
  group_by( id ) |>
  mutate( correct_response = ifelse( trial_info=="Inducer trial" & is.na(rt), 0, correct_response) ) |>
    #' !  Non-responses count as a wrong response & are subject to the exclusion criteria.  !
  summarise( acc = sum(correct_response, na.rm = TRUE) / length( !is.na(correct_response) ) ) |>
  filter( acc < .7 ) |>
  pull( id ) -> loss$exclude_par

loss[["exclude_par_trials"]] <- length( d_ex$rt[d_ex$id == loss$exclude_par] )
loss[["exclude_par_pct"]]    <- length( d_ex$rt[d_ex$id == loss$exclude_par] ) / loss$data_trials * 100

d_ex |> filter( !(id %in% loss$exclude_par) ) -> d_ex
# rt dev+miss
d_ex |>
  mutate(rt = as.integer(rt)) |>
  ungroup() |>
  group_by(id) |>
  mutate(
    rt_crit = ifelse( 
      trial_info == "Diagnostic trial",
      mean( rt ) + sd( rt ) * 2.5,
      NA ),
      retain_trials = ifelse(
        (rt > rt_crit & trial_info == "Diagnostic trial") | # **OR**
          (is.na(rt) & trial_info == "Diagnostic trial"),
        0, 1 ) 
    ) -> d_ex

sum(d_ex$retain_trials == 0) -> loss[["rt_sd_trials"]]
sum(d_ex$retain_trials == 0) / loss$data_trials * 100 -> loss[["rt_sd_pct"]]

d_ex |> 
  filter( retain_trials == 1 ) -> d_ex
# inducers:
d_ex |>
  mutate( valid_trials = case_when( trial_info=="Inducer trial" & correct_response==1 ~ 1,
                                    trial_info=="Inducer trial" & correct_response==0 ~ 0,
                                    trial_info=="Inducer trial" & is.na(correct_response) ~ 0,
                                      # non-responses count as a wrong response
                                    T ~ NA ) ) |>
  fill(valid_trials, .direction = "up") -> d_ex

sum(d_ex$valid_trials==0) -> loss[["inducer_fail_trials"]]
sum(d_ex$valid_trials==0) / loss$data_trials * 100 -> loss[["inducer_fail_pct"]]

d_ex |> 
  filter( valid_trials == 1 ) |> 
  ungroup() -> d_ex
```

## Estimating the expected loss of data (trials)
```{r}
#| include: false
d_ex |>
  filter(trial_info=="Diagnostic trial") |>
  group_by(id) |>
  count() |>
  ungroup() |>
  pull(n) |> 
  mean() -> exp1_remaining_trials
```

After the exclusion criteria, we are left with `r exp1_remaining_trials` trials.

This corresponds to a loss of `r exp1_remaining_trials/240` (or `r exp1_remaining_trials/240*100` percent). We will assume a loss of .75 (75%). 

In other words, whatever post-cue run we decide, we need to estimate an expected loss of about .75. 


## mdiff/sd/d - Experiment 1 outcome values

Calculate the standardized means and standard deviations, and effect sizes from experiment 1. 

(note. this is for the *excluded* data)
```{r}
#| echo: false

d_ex |> 
  ungroup() |>
  filter(!is.na(con)) |>
  summarise(
    rt = mean(rt), 
    pe = 1 - mean(correct_response),
    .by = c(id, con)
  ) |> 
  mutate(
    rt = scale(rt),
    pe = scale(pe)
  ) |>
  pivot_wider(names_from = con, values_from = c(rt,pe)) |>
  summarise(
    rt_d_m = mean(rt_FALSE-rt_TRUE),
    rt_d_sd = sd(rt_FALSE-rt_TRUE),
    rt_d = rt_d_m / rt_d_sd,
    pe_d_m = mean(pe_FALSE-pe_TRUE),
    pe_d_sd = sd(pe_FALSE-pe_TRUE),
    pe_d = pe_d_m / pe_d_sd,
  ) -> exp1_outcome
exp1_outcome
```

## GES - Experiment 1 ANOVA outcome results

```{r}
#| echo: false
d_ex |> 
  filter(!is.na(con)) |>
  summarise(
    .by = c(id, con),
    rt = mean(rt),
    pe = 1 - mean(correct_response)
  ) -> d_ex_sum
d_ex_sum |>
  aov_car(rt ~ con + Error(id/con), data = _) |>
  pluck("anova_table") |>
  as.tibble() |>
  mutate( name = "rt", .before = 1) |>
  rbind( 
    d_ex_sum |>
      aov_car(pe ~ con + Error(id/con), data = _) |> 
      pluck("anova_table") |>
      as.tibble() |>
      mutate( name = "pe",.before = 1)
    ) -> exp1_anova_outcome
exp1_anova_outcome
```

RT has a GES of 0.00848
PE has a GES of 0.117 


# Sampling from exp1: 

## Slice from the START of the experiment: 
```{r}
#| echo: false
map_df(2:90, \(pairs){
  d_ex |>
    filter(inducer_run>0) |>
    slice_head(n = pairs, by = c(id, con)) |>
    summarise(
      .by = c(id, con),
      rt = mean(rt),
      pe = 1 - mean(correct_response)
    ) -> test
  
  test |>
    t.test(rt ~ con, data=_, paired = T) |> 
    tidy() |> mutate(.before=1, n = "rt") |>
    rbind(
      test |> 
        t.test(pe ~ con, data=_, paired=T) |> 
        tidy() |> mutate(.before=1, n="pe")
    ) |> 
    mutate(.before=1, pairs=pairs)
}) -> head_slice_diag_exclu

head_slice_diag_exclu |>
  #pivot_wider(names_from=n, values_from = 3:10) |>
  ggplot(aes(pairs, p.value, col = n)) + 
  geom_line()+
  geom_hline(yintercept=.05)
```

Slicing from the **start** (!) suggest that we need around 20 pairs (i.e., 40 trials) to a achieve significance result.  

## Slice from the END of the experiment
```{r}
#| echo: false
map_df(2:90, \(pairs){
  d_ex |>
    filter(inducer_run>0) |>
    slice_tail(n = pairs, by = c(id, con)) |>
    summarise(
      .by = c(id, con),
      rt = mean(rt),
      pe = 1 - mean(correct_response)
    ) -> test
  
  test |>
    t.test(rt ~ con, data=_, paired = T) |> 
    tidy() |> mutate(.before=1, n = "rt") |>
    rbind(
      test |> 
        t.test(pe ~ con, data=_, paired=T) |> 
        tidy() |> mutate(.before=1, n="pe")
    ) |> 
    mutate(.before=1, pairs=pairs)
}) -> tail_slice_diag_exclu
tail_slice_diag_exclu |>
  #pivot_wider(names_from=n, values_from = 3:10) |>
  ggplot(aes(pairs, p.value, col = n)) + 
  geom_line()+
  geom_hline(yintercept=.05)
```

Slicing from the **end** reveal a similar pattern, but reaches significance later. 


## Randomly sample
```{r rnd sampling}
#| echo: false

if(run_simulations){
  
  cl <- makeCluster(detectCores()*.9)
  
  clusterEvalQ(cl, {
    library(tidyverse)
    library(tidymodels)
  })
  clusterExport(cl, "d_ex")
  
  pblapply(2:90, \(pairs){
    map_df(1:300, \(itr){
      d_ex |>
        filter(inducer_run>0) |>
        slice_sample(n = pairs, by = c(id, con)) |>
        summarise(
          .by = c(id, con),
          rt = mean(rt),
          pe = 1 - mean(correct_response)
        ) -> test
      
      test |>
        t.test(rt ~ con, data=_, paired = T) |> 
        tidy() |> mutate(.before=1, n = "rt") |>
        rbind(
          test |> 
            t.test(pe ~ con, data=_, paired=T) |> 
            tidy() |> mutate(.before=1, n="pe")
        ) |> 
        mutate(
          .before=1, 
          pairs=pairs,
          itr = itr,
        )
    })
  }, cl=cl) |> 
      map_df(~.x) -> rnd_slice_diag_exclu
  
  stopCluster(cl)
  
  rnd_slice_diag_exclu |>
  summarise(
    .by = c(pairs,n),
    power = mean(p.value<.05), 
    across(where(is.double), mean)
  ) -> sum_rnd_slice_diag_exclu

  if(save_simulation_data){
    write_parquet(rnd_slice_diag_exclu, "../data/simulations/exp2_diagnostic_length_sampling_from_exp1_raw.parquet")
    write_parquet(sum_rnd_slice_diag_exclu,"../data/simulations/exp2_diagnostic_length_sampling_from_exp1_sum.parquet")
  }
}
```

```{r vis power}
#| echo: false
#| warning: false

sum_rnd_slice_diag_exclu |>
  ggplot(aes(pairs, power, col = n)) + 
  geom_line()+
  geom_hline(yintercept=.8)+
  geom_vline(xintercept=27)
```

```{r vis p val}
#| echo: false
#| warning: false
sum_rnd_slice_diag_exclu |>
  ggplot(aes(pairs, p.value, col = n)) + 
  geom_line()+
  geom_hline(yintercept=.05)+
  geom_vline(xintercept=27)
```

Randomly sampling suggest that only PE will be significant (at 80% power) with the expected loss of trials (25%) if we use 72 post-cue diagnostic trials (36 pairs -> 27 pairs remaining). At least if we only use 27 participants.


## Randomly sample and increase sample size 
e.g., randomly "sample" participants (fake "ids" based on the data)
```{r}
#| echo: false

if(run_simulations){
  
  cl <- makeCluster(detectCores()*.95)
  
  clusterEvalQ(cl, {
    library(tidyverse)
    library(tidymodels)
  })
  clusterExport(cl, "d_ex")
  
  map_df(27:60, \(samp_s){
    pblapply(2:90, \(pairs){
      map_df(1:300, \(itr){
        
        # We create random "ids" by randomly sampling "samp_s" times,
        # And create a fake id "rnd_id_" 
        map_df(1:samp_s, \(rnd_samp){
          d_ex |>
            filter(inducer_run>0) |>
            filter(!is.na(con)) |>
            slice_sample(n = pairs, by = con) |> 
            mutate(id = paste0("rnd_id_", rnd_samp))
        }) |>
          summarise(
            .by = c(id, con),
            rt = mean(rt),
            pe = 1 - mean(correct_response)
          ) -> test
      
        test |>
          t.test(rt ~ con, data=_, paired = T) |> 
          tidy() |> mutate(.before=1, n = "rt") |>
          rbind(
            test |> 
              t.test(pe ~ con, data=_, paired=T) |> 
              tidy() |> mutate(.before=1, n="pe")
          ) |> 
          mutate(
            .before=1, 
            samp  = samp_s,
            pairs = pairs,
            itr   = itr,
          )
      })
    }, cl=cl) |> 
      map_df(~.x)
  }) -> rnd_extended_slice_diag_exclu
  
  stopCluster(cl)
  
  rnd_extended_slice_diag_exclu |>
    na.omit() |> # omit nas
    summarise(
      .by = c(samp,pairs,n),
      power = mean(p.value<.05), 
      across(where(is.double), mean)
    ) -> sum_rnd_extended_slice_diag_exclu
  
  if(save_simulation_data){
    write_parquet(rnd_extended_slice_diag_exclu, "../data/simulations/exp1_rnd_sampling_inc_samp_size.parquet")
    write_parquet(sum_rnd_extended_slice_diag_exclu, "../data/simulations/exp1_sum_rnd_sampling_inc_samp_size.parquet")
  }
}
```

```{r}
#| include: false
#| warning: false
sum_rnd_extended_slice_diag_exclu |>
  ggplot(aes(samp, power, col=pairs,group=pairs))+
  facet_wrap(~n)+
  geom_line()
```

```{r}
#| echo: false
sum_rnd_extended_slice_diag_exclu |> 
  filter(pairs %in% c(27, 36)) |>
  ggplot(aes(samp, power, col=factor(pairs),group=factor(pairs)))+
  facet_wrap(~n)+
  geom_line()+ 
  geom_hline(yintercept=.8)+
  geom_hline(yintercept=.9, linetype="dashed", col="darkblue")
```

In this plot, we have created hypothetical participants by randomly slicing pairs of congruency from the experiment 1. 
The results suggest that only PE will achieve high power with a reasonable sample size (slighly more than 30). 
On the other hand, response time barely reach 80% power with 36 pairs at 60 participants. 


To futher explore, I move over to simulating data, rather than slicing (drawing from exp1). 


## Simulate exp1 data

### Testing
```{r}
#| echo: false
gen_data <- function(N, pair.obs, eff.rt, sd.rt, eff.pe, sd.pe){
  #' @param N Number of subjects
  #' @param OBS Number of observation per N
  #' @param eff.rt  Effect size of congruency on response time
  #' @param eff.pct Effect size of congruency on proportion of correct trials 

  tibble(
    id  = rep(1:N, each = pair.obs*2),
    con = rep.int( c("con","incon"), N * pair.obs),
    rt  = rnorm(N * pair.obs * 2, 
                0 + as.integer(con=="con") * eff.rt, 
                sd.rt),
    pe  = rnorm(N * pair.obs * 2, 
                0 + as.integer(con=="con") * eff.pe, 
                sd.pe),
  ) 
}
```

Simulate data with exp1 outcomes:

```{r quick sim}
#| echo: false

map_df(1:20, \(x){
  gen_data(
    27,
    x,
    exp1_outcome$rt_d_m,
    exp1_outcome$rt_d_sd,
    exp1_outcome$pe_d_m,
    exp1_outcome$pe_d_sd
  ) -> single_dat_sim

  single_dat_sim |> 
    summarise(
      .by = c(id, con),
      rt_m = mean(rt),
      pe_m = mean(pe)
    ) -> single_dat_calc
  
  sim_aov_rt <- 
    aov_car(rt_m~con + Error(id/(con)), data=single_dat_calc)
  sim_t_rt <- 
    t.test(rt_m~con, data=single_dat_calc , paired=T, alterantive="greater") 
  
  single_dat_calc |>
    pivot_wider(names_from=con, values_from = c(rt_m, pe_m)) |>
    summarise(
      num = x, 
      m = mean(rt_m_con - rt_m_incon),
      sd = sd(rt_m_con - rt_m_incon),
      d = m / sd,
      t_rt_p = sim_t_rt$p.value,
      aov_rt_p = sim_aov_rt$anova_table$`Pr(>F)`,
      aov_rt_ges = sim_aov_rt$anova_table$ges,
    )
})
```

Increasing the amount of observations merely decreases the SD, making all results more significant. 
The only value that is approximating our results is the simulation with 1 (pair of) observation(s). 

For this reason, we need to simulate the amount of observations we have (~ 180 valid trials) and estimate the experiment parameters that create the results found in experiment 1. 

### Finding the best simulation parameters from experiment 1

```{r find exp1 parameters}
#| echo: false
N = 27
OBS = 180/2
eff.rt = .17
eff.pe = .68

library(parallel)
library(arrow)
library(broom)
#library(tidymodels)

if(run_simulations){
  parallel::makeCluster( detectCores()*.9 ) -> cl
  clusterExport(cl, varlist = c("N", "OBS", "eff.rt", "eff.pe"))
   # Initiate libraries in clusters
  clusterEvalQ(cl, {
    library(tidyverse)
    library(pbapply)
    library(lsr)
    library(broom)
  })
  
  pblapply(seq(1,10,.02), \(x){
    
    map_df(1:200, \(itr){
      
      tibble(
        id  = rep(1:N, each = OBS*2),
        con = rep.int( c("con","incon"), N*OBS),
        rt  = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.rt, x),
        pe  = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.pe, x),
      ) |> 
        summarise( 
          .by = c(id, con),
          rt = mean(rt), 
          pe = mean(pe)
        ) -> trans
        
      
      trans |>
        pivot_wider(names_from=con, values_from=c(rt,pe)) |>
        summarise( 
          rt = mean(rt_con-rt_incon),
          rt_sd = sd(rt_con-rt_incon),
          rt_d = rt / rt_sd,
          pe = mean(pe_con-pe_incon),
          pe_sd = sd(pe_con-pe_incon),
          pe_d = pe/pe_sd,
        ) -> val
      
      trans |>
        t.test(rt ~ con, data = _, paired = T) |>
        broom::tidy() -> rt_test
      
      names(rt_test) <- paste0("rt_", names(rt_test)) 
      
      trans |>
        t.test(pe~con, data=_, paired=T) |>
        broom::tidy() -> pe_test
      
      rt_test |>
        cbind(pe_test) |>
        cbind(val) |>
        mutate(itr = itr,
               sd = x) 
    })
  }, cl = cl) |> 
    map_df(~.x) -> finding_exp1_parameter_vals
  
  write_parquet(finding_exp1_parameter_vals, sink = "../data/simulations/finding_parameter_values_for_exp1.parquet")
  
  stopCluster(cl)
}
```

```{r}
#| include: false

finding_exp1_parameter_vals |>
  select(rt:sd, contains("p.value")) |>
  summarise(
    .by = sd, 
    pe_pow = mean(p.value < .05),
    rt_pow = mean(rt_p.value < .05),
    across(everything(), mean)
  )
```

Closest RT parameters: mdiff = .166, sd = 2     (d = .58, pow = 80%).
Closest PE parameters: mdiff = .65,  sd = 7.6   (d = .59, pow = 80%).

That is, these parameter values correspond to the results from experiment 1 *with* 180 trials (or 90 pairs; assuming we have exactly 90 pairs). 

With this, we can simulate data for the experiment *with less trials*. 

Only the these parameter may highlight the problem with lacking trials (i.e., post-cue trial) for the post-cue diagnostic run. 


### Small simulation with parameter estimates

```{r}
#| echo: false

l <- 
list( 
  rt_m  = exp1_outcome$rt_d_m,
  rt_sd = 2,
  pe_m  = exp1_outcome$pe_d_m,
  pe_sd = 7.6
)

if(run_simulations){
  
  makeCluster(detectCores() * .9) -> cl
  clusterExport(cl, c("l", "gen_data"))
  clusterEvalQ(cl, {
    library(tidyverse)
    library(broom)
  })
  
  pblapply(seq(1, 90, 1), \(pairs){
    
    map_df(1:300, \(itr){
      gen_data(27, pairs, l$rt_m, l$rt_sd, l$pe_m, l$pe_sd) |> 
        summarise(
          .by = c(id, con), 
          rt = mean(rt),
          pe = mean(pe)
        ) -> s_d
      
      s_d |>
        pivot_wider(names_from = con, values_from = c(rt,pe)) |>
        summarise(
          rt_m = mean(rt_con-rt_incon),
          rt_sd = sd(rt_con-rt_incon),
          rt_d = rt_m / rt_sd,
          pe_m = mean(pe_con-pe_incon),
          pe_sd = sd(pe_con-pe_incon),
          pe_d = pe_m / pe_sd,
        ) |> 
        cbind(
          t.test(rt ~ con, s_d, paired=T, alternative="greater") |>
            tidy() |> 
            select(2:3) |>
            rename(rt_stat = statistic,
                   rt_p = p.value)
        ) |>
        cbind( 
          t.test(pe ~ con, s_d, paired=T, alternative="greater") |>
            tidy() |> 
            select(2:3) |>
            rename(pe_stat = statistic,
                   pe_p = p.value)
        ) |>
        mutate(
          pairs = pairs,
          itr = itr, 
        ) |>
          select(pairs, itr, starts_with("rt"), starts_with("pe")
        )
    })
    
  }, cl = cl) |> 
    map_df(~.x) -> vary_diagnostic_lengths
  
  stopCluster(cl)
  
  if(save_simulation_data){
    write_parquet(vary_diagnostic_lengths, "../data/simulations/vary_diagnostic_length.parquet")
  }
  
} 
```

```{r summary of the above }
#| include: false
vary_diagnostic_lengths |> 
  summarise(
    .by = pairs,
    rt_pow = mean(rt_p<.05),
    pe_pow = mean(pe_p<.05), 
    across(c(starts_with("rt_"), starts_with("pe_")), mean)
  ) -> sum_vary_diag_len
```

The relative power with respect to the simulated data as calculated from the estimate parameters. 

```{r}
#| echo: false

sum_vary_diag_len |> 
  pivot_longer(c(rt_pow, pe_pow)) |>
  ggplot(aes(pairs, value, col=name))+
  geom_line()+
  scale_x_continuous(breaks=seq(0,90,10))+
  scale_y_continuous(breaks=seq(0,1,.1))+
  geom_hline(yintercept = .8)+
  geom_vline(xintercept = 35, linetype="dashed")+
  geom_vline(xintercept = 35 * .75, linetype="dashed", col="red")
```

According to the simulation, it would suffice with 60 pairs (120 trials) to have 80% power to detect a true effect.

However, assuming we have 70 trials in the post-cue run, we will have 35 (forced) pairs (black dashed). As we will lose 25% of the trials, we are left with 26.25 pairs (red dashed), which has a power close to 50%. 

We might remedy this problem by increasing the sample size. 

### Big simulation with parameter estimates 

```{r}
#| echo: false

l <- 
  list( 
    rt_m  = exp1_outcome$rt_d_m,
    rt_sd = 2,
    pe_m  = exp1_outcome$pe_d_m,
    pe_sd = 7.6
  )

if(run_simulations){
  
  makeCluster(detectCores() * .9) -> cl
  clusterExport(cl, c("l", "gen_data"))
  clusterEvalQ(cl, {
    library(tidyverse)
    library(broom)
  })
  
  pblapply(20:100, \(size){
    
    map_df(seq(1, 90, 1), \(pairs){
      
      map_df(1:300, \(itr){
        gen_data(size, pairs, l$rt_m, l$rt_sd, l$pe_m, l$pe_sd) |> 
          summarise(
            .by = c(id, con), 
            rt = mean(rt),
            pe = mean(pe)
          ) -> s_d
        
        s_d |>
          pivot_wider(names_from = con, values_from = c(rt,pe)) |>
          summarise(
            rt_m = mean(rt_con-rt_incon),
            rt_sd = sd(rt_con-rt_incon),
            rt_d = rt_m / rt_sd,
            pe_m = mean(pe_con-pe_incon),
            pe_sd = sd(pe_con-pe_incon),
            pe_d = pe_m / pe_sd,
          ) |> 
          cbind(
            t.test(rt ~ con, s_d, paired=T, alternative="greater") |>
              tidy() |> 
              select(2:3) |>
              rename(rt_stat = statistic,
                     rt_p = p.value)
          ) |>
          cbind( 
            t.test(pe ~ con, s_d, paired=T, alternative="greater") |>
              tidy() |> 
              select(2:3) |>
              rename(pe_stat = statistic,
                     pe_p = p.value)
          ) |>
          mutate(
            size = size,
            pairs = pairs,
            itr = itr, 
          ) |>
            select(size, pairs, itr, starts_with("rt"), starts_with("pe")
          )
      })
    })
  }, cl = cl) |> 
    map_df(~.x) -> diag_lens_over_samp_sizes

  # summarise  
  diag_lens_over_samp_sizes |>
    summarise(
      .by = c(size, pairs), 
      rt_pow = mean(rt_p<.05), 
      pe_pow = mean(pe_p<.05), 
      across(where(is.double), mean)
    ) -> sum_diag_lens_over_samp_sizes
  
  if(save_simulation_data){
    write_parquet(diag_lens_over_samp_sizes, "../data/simulations/diag_lens_over_sample_sizes.parquet")
    write_parquet(sum_diag_lens_over_samp_sizes, "../data/simulations/diag_lens_over_sample_sizes_sum.parquet")
  }
  
  stopCluster(cl)
} 
```

```{r big simu general overview}
#| echo: false
sum_diag_lens_over_samp_sizes |>
  pivot_longer(c(rt_pow, pe_pow)) |>
  # filter(pairs %in% c(27, 36)) |>
  ggplot(aes(size, value, col=pairs, group=pairs))+
  facet_wrap(~name)+
  geom_line()+
  geom_hline(yintercept=.8)
```

Power of RT over sample size and 36 (72 trials) and 27 pairs (54 trials):
```{r}
#| echo: false

sum_diag_lens_over_samp_sizes |>
  filter(pairs %in% c(27, 36)) |>
  pivot_longer(c(rt_pow, pe_pow)) |>
  ggplot(aes(size, value, col=interaction(pairs, name), group=interaction(pairs,name)))+
  # facet_wrap(~name)+
  geom_line()+
  geom_smooth()+
  geom_hline(yintercept=.8)+
  geom_vline(xintercept=45)
```

By simulating data according to the estimated parameters, we require around 45 participants to achieve 80% power for PE.
RT would require closer to 55 participants. 


```{r}

```


