  ---
title: "Experiment 2 post-cue sampling"
format: docx
lang: en-GB
editor: 
  markdown:
      wrap: 62
---

# Sampling from experiment 1

First we can sample effects from experiment 1.

```{r}
#| include: false
load("../data/trans/exp1_data.rdata")

library(tidyverse)
library(lsr)
library(pbapply)
library(afex)
library(gt)

```

```{r}
#| echo: false
d <-
  exp1_d |>
  dplyr::select(id, trial_info, inducer_run, diagnostic_run, correct_response, rt, con) |>
  mutate(rt = as.numeric( ifelse(rt=="null", NA, rt)) ) |>
  filter(trial_info == "Diagnostic trial" | trial_info == "Inducer trial") |>
  filter(!is.na(correct_response))
```

```{r}
#| echo: false
d |>
  filter(diagnostic_run <5) |>
  dplyr::group_by(id, con) |>
  summarise(rt = mean(rt, na.rm=T),
            pct = mean(correct_response,na.rm=T)) |>
  pivot_longer(c(rt,pct)) |>
  ggplot(aes(con, value))+
  facet_wrap(~name, scales ="free")+
  stat_summary()
```
Congruency present for raw data.


```{r}
#| echo: false
diag_trials <-
  d |>
  filter(diagnostic_run<17) |>
  ungroup()
```


First, I will test how many diagnostic trials we need to find a sig difference, based on the results we already have. 
```{r}
map_df(seq_along(1:16), \(x){
  diag_trials |>
    filter(diagnostic_run<x) |>
    group_by(id, con) |>
    summarise(
      rt = mean(rt),
      pct = mean(correct_response)
    ) |> ungroup() |>
    summarise(
      n = x,
      est = t.test(rt~con, paired=T)$estimate,
      t = t.test(rt ~ con, paired=T)$statistic,
      p = t.test(rt ~ con, paired=T)$p.value,
      df = t.test(rt ~ con, paired=T)$parameter,)
}) -> n_back_test
n_back_test |>
  ggplot(aes(n, p))+
  geom_line()+
  geom_hline(yintercept=.05)
```
Sampling from all blocks, across all participants, we appear to need between 8-10 trials (per block) to find a significant congruency effect. 


Now I will attempt to see how many diagnostic trials are necessary to find the congruency effect through random sampling of the data from experiment 1. 
```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
stat_test <- function(size){
  # n = size 
  
  slice_sample(diag_trials, by = c(id, con), n = size) |>
    group_by(id, con) |>
    summarise(l = length(rt),
              rt = mean(rt),
              pct = mean(correct_response)) |>
    ungroup() |>
    pivot_wider(names_from=con, values_from=c(l, rt,pct)) |>
    reframe(
      n = size,
      rt_con = mean(rt_TRUE, na.rm=T),
      rt_incon = mean(rt_FALSE, na.rm=T),
      rt_est  = mean(rt_FALSE-rt_TRUE, na.rm=T),
      rt_t  = t.test(rt_FALSE, rt_TRUE, paried=T, na.rm=T)$statistic,
      rt_p  = t.test(rt_FALSE, rt_TRUE, paried=T, na.rm=T)$p.value,
      rt_d = cohensD(rt_FALSE, rt_TRUE, method="corrected"),
      pct_con = mean(pct_TRUE, na.rm=T),
      pct_incon = mean(pct_FALSE, na.rm=T),
      pct_est  = mean(pct_FALSE-pct_TRUE, na.rm=T),
      pct_t  = t.test(pct_FALSE, pct_TRUE, paried=T, na.rm=T)$statistic,
      pct_p  = t.test(pct_FALSE, pct_TRUE, paried=T, na.rm=T)$p.value,
      pct_d = cohensD(pct_FALSE, pct_TRUE, method="corrected")
    )
}
map_df(1:50, \(x){
  stat_test(x)
}) -> sample1 

sample1 |>
  ggplot(aes(n, rt_p))+
  geom_line()
```

Generally this appears to be unstable. The increase in diagnostic trials does not appear to change the significance at all. Weird.

To solve this, we can repeat each sampling of a particular size a couple of times to average the differences.
```{r}
#| echo: false
#| error: false
#| warning: false
#| message: false
rep_test <- function(maxdiag, rep){
  # range = number of diagnostics
  # rep = how many times we repeat the same sampling

  map_df(1:maxdiag, \(x){
    # for each diag we repeat the diag length "rep" times
    map_df(1:rep, \(y){
      stat_test(x)
    })
  })
}
rep_test(50, 30) -> sample2
  # From 1 to 50 diagnostic trials, repeat each 20 times. 
sample2 |>
  ggplot(aes(n, rt_p))+
  stat_summary()
```


```{r}
sample2 |>
  ggplot(aes(n, rt_d))+
  stat_summary()
```
This did not appear to solve the problem at all. 

I suspect there is some unequalness with the sampling... 
We can move over to generated data to see how increased samples "should" trend the data towards more significant differences. 

# Generating data

Generate normal distributions with a mean of 0 and a standard deviation of 1.
Effect.size adjusts the effect on the congruent trials by "eff.size" difference.
For our purposes here "rt" and "pct" can be on the same scale.
```{r}
gen_sample <- function(N, OBS, eff.rt, eff.pct){
  #' @param N Number of subjects
  #' @param OBS Number of observation per N
  #' @param eff.rt  Effect size of congruency on response time
  #' @param eff.pct Effect size of congruency on proportion of correct trials 

  tibble(
    id = rep(1:N, each = OBS*2),
    con = rep.int( c("con","incon"), N*OBS),
    rt = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.rt, 1),
    pct = rnorm(N*OBS*2, 0 + as.integer(con=="con") * eff.pct, 1),
  ) 
}
```


We can then generate random data related to each effect size with different diagnostic lengths.
In this case, effect size in steps of .05 from 0 to .5. Moreover, we use 30 to 70 diagnostic trials.
60 trials would correspond to an average of 2.5 trials in each block, and what would correspond to a range of 0 to 5 trials (post-cue trials).

```{r}
#| echo: false

library(pbapply)
library(parallel)

# Create a "cluster" to simulate faster
# Take half the computers cores 
makeCluster( floor(detectCores() - detectCores() * 1/2) ) -> cl
# Export the local function
clusterExport(cl, varlist = c("gen_sample"))
# Initiate libraries in clusters
clusterEvalQ(cl, {
  library(tidyverse)
  library(pbapply)
  library(lsr)
})


# For each different effect size (*y*)...
pblapply(seq(0,.75, .05), \(y){
  
  # Sequence over each sample size (*i*)...
  map_df(10:90, \(i){
    # Sequence diagnostic lengths (*x*)....
    map_df(30:120, \(x){
      
      # Generate...
      gen_sample(i, x, y/4, y) |> 
        group_by(id) |> 
        mutate( n = rep(1:(length(rt)/2), each=2) ) |> 
        pivot_wider(names_from=con, values_from = c(rt,pct))  |> 
        ungroup() |> 
        summarise(
          eff = y, 
          size = i,
          dia.len = x,
          rt.con = mean(rt_con),
          rt.incon = mean(rt_incon),
          rt.di = mean(rt_incon-rt_con),
          rt.p = t.test(rt_incon, rt_con, paired=T)$p.value,
          rt.t = t.test(rt_incon, rt_con, paired=T)$statistic,
          rt.d = cohensD(rt_incon, rt_con, method = "paired"),
          pct.con = mean(pct_con),
          pct.incon = mean(pct_incon),
          pct.di = mean(pct_incon-pct_con),
          pct.p = t.test(pct_incon, pct_con, paired=T)$p.value,
          pct.t = t.test(pct_incon, pct_con, paired=T)$statistic,
          pct.d = cohensD(pct_incon, pct_con, method = "paired"),)
    })
  })
}, cl = cl) |>
  map_df(~.x) -> gen_data

# save(gen_data, file = "../data/sim/diagnostic_length_simulation.rdata")

# stop cluster
stopCluster(cl)
```


```{r}
gen_data |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(size, value, col = eff, group=eff ))+
  facet_wrap(~name, scales="free")+
  stat_summary(geom="line")+
  geom_smooth(method="lm")+
  scale_y_log10()+
  geom_hline(yintercept=0.05)+
  geom_vline(xintercept=60)
```

In this graph we see a clear increase in p for increasing diagnostic lengths. 

```{r}
#| echo: false
gen_data |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(size, rt.d, col = eff, group=eff ))+
  stat_summary(geom="line")+
  geom_smooth(method="lm", alpha =.2)
```

Similarly, the effect size seems to be rather stable for all diagnostic lengths. 


From the generated data, we can calculate the power for each effect size: 

```{r}
#| echo: false

gen_data |>
  group_by(eff) |>
  summarise(power.rt = mean(rt.p<.05), power.pct = mean(pct.p<.05)) |>
  gt()
```


Let us sample multiple times with 60 diagnostic trials:

```{r}
#| echo: false
#| message: false
#| warning: false
map_df(seq(0,.5, .05), \(y){
  map_df(1:1000, \(x){
    gen_sample(27, 60, y) |>
      summarise(
        eff = y,
        count = x,
        size = 60, 
        rt.con = mean(rt_con, na.rm=T),
        rt.incon = mean(rt_inc, na.rm=T),
        rt.est  = mean(rt_inc-rt_con, na.rm=T),
        rt.t  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$statistic,
        rt.p  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$p.value,
        rt.d = cohensD(rt_inc, rt_con, method="corrected"),
        pct.con = mean(pct_con, na.rm=T),
        pct.incon = mean(pct_inc, na.rm=T),
        pct.est  = mean(pct_inc-pct_con, na.rm=T),
        pct.t  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$statistic,
        pct.p  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$p.value,
        pct.d = cohensD(pct_inc, pct_con, method="corrected")
      )
  })
}) -> gen_data2


gen_data2 |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(count, value, col = eff, group=eff ))+
  facet_wrap(~name, scales="free")+
  stat_summary(geom="line", alpha = .4)+
  geom_smooth(method="lm")+
  scale_y_log10()+
  geom_hline(yintercept=0.05)
```

This yields a very nicely coloured graph. At any rate, we can see that there is some variation around each effect size, but that it is generally stable over time.

From this, we can calculate the power for each effect size:
```{r}
#| echo: false

gen_data2 |>
  group_by(eff) |>
  summarise(power.rt = mean(rt.p<.05), power.pct = mean(pct.p<.05)) |>
  gt()
```

With an effect size of at least .10, we will get a power of 80%.

In line with our results, we found .17 for rt and .70 with accuracy. 
This means we should be able to detect both response time and accuracy with a high than 98% power. 



Testing slightly less trials, due to loss at the first inducer block: 
```{r}
#| echo: false
#| message: false
map_df(seq(0,.5, .05), \(y){
  map_df(1:1000, \(x){
    gen_sample(27, 55, y) |>
      summarise(
        eff = y,
        count = x,
        size = 55, 
        rt.con = mean(rt_con, na.rm=T),
        rt.incon = mean(rt_inc, na.rm=T),
        rt.est  = mean(rt_inc-rt_con, na.rm=T),
        rt.t  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$statistic,
        rt.p  = t.test(rt_inc, rt_con, paried=T, na.rm=T)$p.value,
        rt.d = cohensD(rt_inc, rt_con, method="corrected"),
        pct.con = mean(pct_con, na.rm=T),
        pct.incon = mean(pct_inc, na.rm=T),
        pct.est  = mean(pct_inc-pct_con, na.rm=T),
        pct.t  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$statistic,
        pct.p  = t.test(pct_inc, pct_con, paried=T, na.rm=T)$p.value,
        pct.d = cohensD(pct_inc, pct_con, method="corrected")
      )
  })
}) -> gen_data3

gen_data3 |>
  filter(eff>.1) |>
  mutate(eff = factor(as.character(eff))) |>
  pivot_longer(c(rt.p, pct.p)) |>
  ggplot(aes(count, value, col = eff, group=eff ))+
  facet_wrap(~name, scales="free")+
  stat_summary(geom="line", alpha = .4)+
  geom_smooth(method="lm")+
  scale_y_log10()+
  geom_hline(yintercept=0.05)
```

```{r}
#| echo: false

gen_data3 |>
  group_by(eff) |>
  summarise(power.rt = mean(rt.p<.05), power.pct = mean(pct.p<.05)) |>
  gt()
```

Still results in rather similar outcomes. A solid > 98% power at an effect size of .15. 


# Congruency check 
A last notion relates to whether we need to or should force an equal number of diagnostic trials. 

```{r}
#| echo: false
#| message: false
d |>
  filter(!is.na(con)) |>
  group_by(id, con) |>
  count() |>
  mutate(trials = n - 30) |>
  ggplot(aes(trials))+
  geom_histogram(alpha = .4)+
  geom_histogram(aes(x=n, fill="all data"), alpha=.4)+
  scale_x_continuous(breaks = seq(5,200, 5))
```

In relation to the simulated data above, removing 30 trials does not lead to problems with regards to finding a congruency effect. 
With at least 75 trials for some conditions. 

The general conclusion from this is that we do not need to force an equal number of right/left diagnostic trials for the **pre-cue**, but to be sure we receive enough trials for the **post-cue** we will force that to be equal.
